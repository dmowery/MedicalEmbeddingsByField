\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp-ijcnlp-2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}
%\usepackage{hltnaacl04}
%\usepackage{hyperref}
\usepackage{pgfplotstable}
\usepackage{csvsimple}
\usepackage{pgfplotstable}
\usepackage{color}
\usepackage{boldline}
%\usepackage{listings}

\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\textbf{\color{blu}#1}}}

\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}

\definecolor{ora}{rgb}{1,0.6,0}
\def\ora#1{{\textbf{\color{ora}#1}}}


\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Comparing the Intrinsic Performance of Clinical Concept Embeddings by their Field of Medicine}

\author{John-Jose Nunez\\
  Depts. of Psychiatry and Computer Science, UBC\\
  {\tt jjnunez@cs.ubc.ca} 
}
\date{}

\begin{document}
\maketitle
\begin{abstract}
	Pre-trained word embeddings are becoming increasingly popular for natural language-processing tasks. This includes medical applications, where embeddings are trained for clinical concepts using specific medical data. Recent work continues to improve on these embeddings. However, no one has yet sought to determine whether these embeddings work as well for one field of medicine as they do in others. In this work, we use intrinsic methods to evaluate embeddings from the various fields of medicine as defined by their ICD-9 systems. We find significant differences between fields, and motivate future work to investigate whether extrinsic tasks will follow a similar pattern. 
\end{abstract}

\section{Introduction}

The application of natural language processing (NLP) and machine learning to medicine presents an exciting opportunity for tasks requiring prediction and classification, such as predicting the risk of suicide after a patient is discharged from hospital ~\cite{mccoyImprovingPredictionSuicide2016}. A common method across NLP for such tasks is to use high-dimensional vector word representations. These word embedding include the popular \emph{word2vec} system \cite{mikolovEfficientEstimationWord2013} which was initially trained on general English text, using a skip-gram model on a Google News corpus.

Due to considerable differences between the language of medical text and general English writing, prior work has trained medical embeddings using specific medical sources. Generally, these approaches have trained embeddings to represent medical concepts according to their `clinical unique identifiers' (CUIs) in the Unified Library Management System (ULMS) \cite{bodenreiderUnifiedMedicalLanguage2004}. Words in a text can then be mapped to these CUIs \cite{yuShortIntroductionNILE2013}. Various sources have been used, such as medical journal articles, clinical patients records, and insurance claims \cite{devineMedicalSemanticSimilarity2014}, \cite{minarro-gimenezExploringApplicationDeep2014},  \cite{choiLearningLowDimensionalRepresentations2016}.  

Prior authors have sought to improve the quality of these embedding, such as using different training techniques or more training data \cite{beamClinicalConceptEmbeddings2018}. In order to judge the quality of these embeddings, they have primarily used evaluation methods quantifying intrinsic qualities, such as their ability to predict drug-disease relations noted in the National Drug File - Reference Terminology (NDF-RT) ontology \cite{minarro-gimenezExploringApplicationDeep2014}, or whether similar types of clinical concepts had cosine similiar vectors  \cite{choiLearningLowDimensionalRepresentations2016}.

To date, these embeddings have been both trained and evaluated on general medical data, from all fields of medicine. It is unclear how well such embeddings perform for a specific field of medicine. For example, we can consider psychiatry, the field of medicine concerned with mental illnesses such as depression or schizophrenia. Prior work has shown that psychiatric symptoms are often described in a long, varied, and subjective manner \cite{forbushSittingPinsNeedles2013} which may present a particular challenge for training these embeddings and NLP tasks generally.

As these pre-trained embeddings may be increasingly be used for down-stream NLP tasks in specific fields of medicine, we seek to determine whether embeddings from one field perform relatively well or poorly relative to others. Specifically, we aim to follow prior work using intrinsic evaluation methods, comparing the geometric properties of embeddings vectors against others given known relationships. This will offer a foundation for future work that may compare the performance of extrinsic NLP tasks in different medical fields. Finding relative differences may support that certain medical fields would benefit from embeddings trained on data specific to their field, or using domain adaptation techniques as sometimes used in the past \cite{yuRetrofittingConceptVector2017}. 


\section{Methods}

\subsection{Sets of Embeddings}

\begin{table*}[h!]
	
	\begin{center}
		\begin{tabular}{lcccl} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
			Name & Dimension & Number & Number of Training Data& Type of Training Data \\
			\hlineB{4}
			DeVine200 & 200 & 52,102 & 17k + 348k &clinical narratives, journal abstracts\\
			\hline
			ChoiClaims300 & 300 & 14,852& 4m&health insurance claims\\
			\hline
			ChoiClinical300 & 300 & 22,705&20m& clinical narratives\\
			\hline
			BeamCui2Vec500 & 500 & 109,053&60m + 20m + 1.7m& claims, narratives, full journal texts\\
		\end{tabular}
	\end{center}
	\caption{Characteristics of the embeddings compared, including the name referred, the embedding dimensions, the number of embeddings in the dataset, and the type of data used to train them.}
	\label{tab:embed}
\end{table*}

We sought to compare a variety of clinical concept embeddings trained on medical data. Table ~\ref{tab:embed} contains details of the sets compared in this project, all of which are based on \emph{word2vec}. We obtained DeVine200 \cite{devineMedicalSemanticSimilarity2014}, ChoiClaims300, and ChoiClinical300 \cite{choiLearningLowDimensionalRepresentations2016} all from the \href{https://github.com/clinicalml/embeddings}{later's Github}. We downloaded BeamCui2Vec500 \cite{beamClinicalConceptEmbeddings2018} from \href{https://figshare.com/s/00d69861786cd0156d81}{this site}. We were unable to obtain other sets of embeddings mentioned in the literature \cite{minarro-gimenezExploringApplicationDeep2014}, \cite{zhangAdaptingWordEmbeddings2018} \cite{xiangTimesensitiveClinicalConcept2019}.


\subsection{Determining a Field of Medicine's Clinical Concepts}
A clinical concept's corresponding field of medicine is not necessarily obvious. In order to have an objective and unambiguous classification, we utilized the ninth revision of the International Statistical Classification of Diseases and Related Health Problems (ICD-9) \cite{sleeInternationalClassificationDiseases1978}. This is a widely used system of classifying medical diseases and disorders, dividing them into seventeen chapters representing medical systems/categories such as mental disorders, or disease of the respiratory system. While the 10th version is available, we chose this version based on prior work using it, and the pending release of the 11th version. We will use these ICD9 systems to define the different medical fields. 

We determined a CUI's field of medicine according to a CUI-to-ICD9 dictionary available from the UMLS \cite{bodenreiderUnifiedMedicalLanguage2004}. We consider pharmacological substance related to a field of medicine system if it treats or prevents a disease with an ICD9 code within a particular ICD9 system. We determine this by using the NDF-RT dictionary, which maps CUIs of substances to the CUIs of conditions they treat or prevent, and then convert these CUIs to the ICD9 systems as before.  As such, A CUI repersenting a drug may have multiple ICD9 systems and therefore medical fields. 

\subsection{Evaluation Methods}

We sought to compare multiple methods for evaluating the quality of a medical field's embeddings based on prior work. We were unable to use Yu et al's \shortcite{yuRetrofittingConceptVector2017} method, based on comparing the correlation of vector cosine similarity against human judgements from the UMNSRS-Similarity dataset \cite{pakhomovSemanticRelatednessSimilarity2018} due to there being too few examples across many medical fields. 

\paragraph{Medical Relatedness Measure}(MRM)

This method from Choi et al \shortcite{choiLearningLowDimensionalRepresentations2016} is based on quantifying whether concepts with known relations are neighbours of each other.  They use known relationships between drugs and the diseases they treat or prevent, and also the relations between diseases that are grouped together in the CCS hierarchical groupings, a classification from the Agency for Healthcare Research and Quality \cite{ClinicalClassificationsSoftware}. The scoring utilizes Discounted Cumulative Gain, which attributes a diminishing score the further away a known relationship is found if within \emph{k} neighbours. 

In our implantation, we calculate the MRM based on the `course' groupings from CCS drug hierarchies. Scores are calculated for CUIs that represent diseases with a known ICD9 code. The mean MRM is then calculated for all CUIs within a given ICD9 system. The implementation was adapted from Python 2.7 code available from the original author's \href{https://github.com/clinicalml/embeddings}{Github}.


\paragraph{Medical Conceptual Similarity Measure}(MCSM) The other method used by Choi et al's work evaluates whether embeddings known to be of a particular set are clustered together. They use conceptual sets from the UMLS such as `pharmacologic substance' or `disease or syndrome'.  Discounted Cumulative Gain is again used, based on whether a CUI has other CUIs of its set within \emph{k} neighbours. 

We reimplement this method, but instead of using the UMLS conceptual sets, we create sets from the ICD9 systems, again giving a score to neighbours that are diseases or drugs from the same field of medicine/ICD9 system. Again, this was adapted from code from Choi et al's Github.  

%\paragraph{Correlation with UMNSRS Similarity}(SimCor)
%\cite{yuRetrofittingConceptVector2017} investigate whether the cosine similarity of embedding vectors are correlated with human judgements of similarity.  contains around 500 similarity ratings between medical concepts as rated by eight medical residents. Yu et all then compute a Spearman rank correlation between the cosine similarities and the UMNSRS-Similarity ratings.
%
%We repeated the above, calculating a medical system's mean correlation based on the Spearman rank correlation of all pairing that contain at least one disease with an ICD9 code in its system, or a drug that treats or prevents a disease in the system. This was implemented from scratch in Python, and we used the \href{https://conservancy.umn.edu/handle/11299/196265}{UMN SRS modified similarity dataset}.

\paragraph{Significance against Bootstrap Distribution}(Bootstrap)
Beam et al \shortcite{beamClinicalConceptEmbeddings2018} also evaluate how well known relationships between concepts are represented by embedding vector similarity. For a given known relation, they generate a bootstrap distribution by randomly calculating cosine similarities between embedding vectors of the same class (eg. a random drug and disease when evaluating  drug-disease relations). For a given known relation, they consider that the embeddings produced an accurate prediction if their cosine similarity is within the top 5\%, the equivalent of p<0.05 for a one-sided t-test. 

Our implementation considers the may-treat or may-prevent known relationships from the NDF-RT dataset. We calculate the percentage of known relations for drug-disease pair within each medical field. Beam et al have not yet made their code publicly available, so we reimplemented this technique in Python. 

\paragraph{System Vector Accuracy}(SysVec)
We implement a new, simple method to evaluate a medical field's embeddings. A representative vector is calculated for each medical field/ICD9 system by calculating the mean of normalized embeddings of a field's diseases. We then calculate the percentage of drugs known to treat or prevent a disease in each system whose vectors are most similar (by cosine similarity) to the relevant system vector. For example, we would expect the CUI for `fluoxetine', an anti-depressant, to be most similar to the Mental Disorders system vector. 

Some drugs treat or prevent diseases in \emph{n} multiple systems. For a given medical field, such a drug is considered being accurately predicted if the system's centroid is amongst the \emph{n} most similar centroids. We implemented this in Python. 

\subsection{Comparing Scores}

\paragraph{Comparing Sets of Embeddings}: We calculated the mean scores for an embedding set, only including embeddings with corresponding ICD9 values and present in all of the compared sets. For the MCSM and MRM scores, we conducted two-way paired t-tests between the scores from each embedding set, adjusted with the Boneferroni correction. For the binary Bootstrap and SysVec scores, we judged statistical significance by calculating z-scores and their corresponding Boneferroni corrected p-values. 

A negative control set of embeddings was constructed by taking the embeddings from Beam et al \shortcite{beamClinicalConceptEmbeddings2018} and randomly arranging which clinical concepts an embedding corresponds to. 

\paragraph{Comparing Fields of Medicine}: As the embeddings from Beam et al \shortcite{beamClinicalConceptEmbeddings2018} are most recent, trained on the most data, and have significantly higher scores than the other embeddings compared, we used these embeddings to compare scores from the different fields of medicine. This set also contained the most embeddings, allowing more embeddings from each field to be compared. 

We sought to determine whether a field of medicine's embeddings were significantly worse or better than the average. As such, for each field of medicine we calculated the mean score from each evaluation method. We then used statistical tests to compare a field's scores from a given evaluation method with the same scores from all other fields. For MCSM and MRM scores we used two-tailed t-tests, and for Bootstrap and Sysvec, z-scores, all corrected with the Bonferroni correction. 

To aggregate a medical field's results, we calculated a `net score' by taking how many of the four method's scores were significantly above the mean, minus how many were significantly below. We found this more interpretable than other aggregate methods such as combining normalized scores. 

\section{Results}


\subsection{Differences Between Sets of Embeddings}
Comparing the sets of embeddings (Table \ref{tab:allsystemresults}) shows consistent differences. BeamCui2Vec500's scores are the highest across all methods, and this difference is very significant, with p-value $\ll 10^-5$ after Bonferonni correction. The ChoiClaims300 embeddings seem next best, and the remaining sets still have much higher scores than those of the negative control. 


\begin{table*}[h]
	\begin{center}
		\label{tab:allembedresults}
		\begin{tabular}{l|c|c|c|c}
			Embedding Set &MRM 	        &MCSM              &Bootsrap 	  &SysVec \\
			\hlineB{4}
			Negative Control& 0.02 & 1.24 & 0.05 & 0.35 \\	            
			DeVine200       & 0.24 & 5.14 &	0.27 & 0.79 \\
			ChoiClaims300   & 0.43 & 5.34 &	0.42 & 0.80 \\ 
			ChoiClinical300	& 0.33 & 4.49 &	0.42 & 0.74 \\
			BeamCui2Vec500	& 0.52 & 6.39 & 0.67 & 0.90 \\
		\end{tabular}
		\caption{Mean scores for embedding sets for each evaluation method. See Methods section for abbreviations}
	\end{center}
\end{table*}


\subsection{Differences Between Medical Systems}

Differences are also observed between embeddings from the various fields of medicine as represented by the ICD-9 systems ~\ref{tab:allsystemresults}. For instance, embeddings related to Infections and Parasitic Disease have scores significantly above the mean for all four evaluation methods, while those of Symptoms, Signs, and Ill-defined Conditions are significantly below for all three. Due to a smaller number of documented drug-disease relationships across two medical fields, scores were not calculated with those methods based on such.  

\begin{table*}[!h]
	\begin{tabular}{lccccc}
		ICD-9 Systen                                                                                                          & MRM           & MCSM           & Bootstrap     & SysVec        & 	Net Score \\
		\hlineB{4}
		All Systems (Negative Control)                                                                                        & 0             & 1.08           & 0.04          & 0.25          & -                                \\
		\hline
		All Systems                                                                                                           & 0.55          & 8.07           & 0.89          & 0.63          & -                                \\
		\hline
		Infectious and Parasitic Diseases       & \textbf{0.45} & \textbf{7.72}  & \textbf{0.93} & \textbf{0.92} & +4                               \\
		\hline
		Neoplasms                                                                                                             & \textbf{0.62} & \textbf{9}     & 0.94          & 0.55          & +2                               \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Endocrine, Nutritional and Metabolic \\Diseases,  and Immunity Disorders\end{tabular} & \textbf{0.44} & \textbf{5.64}  & 0.89          & 0.53          & -2                               \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Diseases of the Blood \\ and Blood-forming Organs\end{tabular}                            & \textbf{0.31} & \textbf{4.36}  & 0.82          & 0.79          & -2                               \\
		\hline
		Mental Disorders                                                                                                      & 0.53          & \textbf{9.34}  & 0.96          & \textbf{0.83} & +2                               \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Diseases of the Nervous System \\ and Sense Organs\end{tabular}                           & \textbf{0.76} & \textbf{8.44}  & 0.87          & \textbf{0.33} & +1                               \\
		\hline
		Diseases of the Circulatory System                                                                                    & 0.59          & 8.12           & \textbf{0.96} & \textbf{0.72} & +2                               \\
		\hline
		Diseases of the Respiratory System                                                                                    & \textbf{0.36} & \textbf{5.85}  & 0.94          & \textbf{0.82} & +1                               \\
		\hline
		Diseases of the Digestive System                                                                                      & \textbf{0.61} & 7.93           & \textbf{0.77} & 0.62          & 0                                \\
		\hline
		Diseases of the Genitourinary System                                                                                  & \textbf{0.61} & \textbf{6.82}  & 0.86          & 0.58          & 0                                \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Complications of Pregnancy, \\ Childbirth, and the Puerperium\end{tabular}                & \textbf{0.51} & \textbf{10.27} & -             & -             & 0                                \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Diseases of the Skin \\ and Subcutanous Tissue\end{tabular}                               & \textbf{0.37} & \textbf{5.1}   & 0.81          & 0.58          & -2                               \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Diseases of the Musculoskeletal \\System and Connective Tissue\end{tabular}              & \textbf{0.47} & 8.22           & 0.88          & \textbf{0.29} & -2                               \\
		\hline
		Congenital Anomalies                                                                                                  & 0.5           & \textbf{6.24}  & 0.73          & 0.73          & -1                               \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Certain Conditions Originating\\ in the Perinatal Period\end{tabular}                    & \textbf{0.48} & \textbf{9.84}  & -             & -             & 0                                \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Symptoms, Signs,  and \\ Ill-defined   Conditions\end{tabular}                           & \textbf{0.26} & \textbf{2.68}  & \textbf{0.77} & 0.56          & -3                               \\
		\hline
		Injury and Poisoning                                                                                                  & \textbf{0.59} & \textbf{9.09}  & \textbf{0.75} & \textbf{0}    & 0                               
	\end{tabular}
	\caption{Comparison of mean scores using different evaluation methods for the fields of medicine as represented by their ICD-9 system. Significant differences below or above the mean of all other systems are bold (p-value \textless 0.05 after Boneferroni correction). Net score is the number of these significant differences above that mean minus the number below. A system's  score is not calculated if there are fewer than ten examples for a method. See Methods section for evaluation method abbreviations}
	\label{tab:allsystemresults}
\end{table*}


\section{Discussion and Future Direction}
To our knowledge, this is the first investigation into whether clinical concept embeddings from a given field of medicine relatively good or bad compared to others. We conducted this investigation comparing available sets of such embeddings, using a variety of previously described intrinsic evaluation methods in addition to a new one. Given that one set of embeddings performed better than others, we used this set to compare the different fields of medicine, and found significant results between various fields. 

The superior performance of one set of embeddings - those from Beam et al \shortcite{beamClinicalConceptEmbeddings2018} - are consistent with the depth and breadth of data used to train these embeddings. Training used three different types of data, including that from health insruance claims, clinical narratives, and full texts from medical journals. The number of data was also much larger the the other sets. Our work validates their findings that their embeddings offer the best performance. However, it would be interesting to also consider the recent clinical concept embeddings developed by \cite{xiangTimesensitiveClinicalConcept2019}. They use a similar number of data (50 million) as Beam et al, using a large dataset from an electronic health records, and use a novel method to incorporate time-sensitive information. At the time of submission, we were unable to obtain their embeddings, and so leave this comparison to future work.

Examining the differences between fields of medicine, we note that the poor performance of embeddings from the system ``Symptoms, Signs, and Ill-defined Conditions" may support validity of the results. This collection of miscellaneous medical conditions would not be expected to have the intrinsic vector similarity and cohesion evaluated by our evaluation methods.   

Further work may explore why the other systems have varied performance. We wonder if the observed results correlate with possible distinctiveness of the various medical fields. For example, the best performing system was ``Infectious and Parasitic Diseases". The conditions in this field are often unambiguous - a pathogen like \emph{influenza virus} has little other meaning - and the drugs used for these diseases tend to be similarly specific. On the other hand,  poorly performing systems such as ``Diseases of the Skin and Subcutaneous Tissue" and "Diseases of the Musculoskeletal Systems and Connective Tissue" often utilize immunosuppressant medications that are used across many fields of medicine. Future work could investigate this conjecture by comparing scores when restricting what clinical concepts are compared, such as only common or distinct medications. 

This work evaluated embeddings using intrinsic measures of embedding quality. This presents some advantages, but also the most obvious limitations and direction for future work. These intrinsic methods allowed a consistent evaluation to be carried out between medical fields, and allowed a wide variety of embeddings from a given field to be compared. The methods all evaluate qualities that well-trained embeddings should have, though still represent artificial use-cases. Evaluating these embeddings on extrinsic, down-stream tasks may provide more applicable comparisons. However, these tasks will need to be comparable and available for multiple medical fields. For instance, the recent work by Xiang et al \shortcite{xiangTimesensitiveClinicalConcept2019} compared embeddings trained by different methodologies on a task predicting the onset of heart failure \cite{rasmyStudyGeneralizabilityRecurrent2018}. This would be an appropriate task to judge embeddings from ``Diseases of the Circulatory System'';  others would be needed for other systems. We  also plan to investigate the validity of these intrinsic evaluation methods by comparing them to extrinsic results. 

Another future direction could be to investigate what could be done to improve performance in the fields with lower scores. For instance, Zhang et al \shortcite{zhangAdaptingWordEmbeddings2018} used domain adaptation techniques for psychiatric embeddings, and this could instead be carried out for those systems we identified as doing poorly. Alternatively, one could also train embeddings solely on data from one field of medicine and investigate how this affects performance. 


\bibliographystyle{acl_natbib}
\bibliography{my_library}


\end{document}
