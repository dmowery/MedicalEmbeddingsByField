%
% Based on File hlt-naacl2004.tex
%
\documentclass[10pt]{article}
\usepackage{hltnaacl04}
\usepackage{times}
\usepackage{latexsym}
\usepackage{hyperref}
\usepackage{pgfplotstable}
\usepackage{csvsimple}
\usepackage{pgfplotstable}
\usepackage{color}

\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}

\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}

\definecolor{ora}{rgb}{1,0.6,0}
\def\ora#1{{\color{ora}#1}}


\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{How does the Performance of Embeddings Trained on General Medical Text Vary by Field of Medicine?}

\author{John-Jose Nunez\\
  Depts. of Psychiatry and Computer Science, UBC\\
  {\tt jjnunez11@gmail.com} 
}
\date{}

\begin{document}
\maketitle
%\begin{abstract}
%\end{abstract}

\section{Introduction}

\subsection{Background}

%%6-7 pages including max 1 page references

%%DESCRIPTION OF PROBLEM EG NLP TASK, CORPUS
%%RELATED WORK

The application of natural language processing and machine learning to medicine presents an exciting opportunity for tasks requiring prediction and classification, such as predicting the risk of suicide after a patient is discharged from hospital \cite{mccoyImprovingPredictionSuicide2016}. A common approach is to convert the unstructured text produced by clinical interactions into low-dimension vector representations which can fed into these algorithms. These vectorizations are produced by training models on large unlabelled corpora. For example, the popular \emph{word2vec} system \cite{mikolovEfficientEstimationWord2013} initially trained embeddings using a skip-gram model, training a vector for a target word based on what words are found within a window near it. It was initially trained on a Google News corpus containing around six billion tokens. Due to considerable differences between the language of medical text and general English writing, prior work has trained medical embeddings using specific medical sources. 

Recent approaches in this vein include De Vine et al \shortcite{devineMedicalSemanticSimilarity2014} which trained embeddings for medical concepts in the Unified Library Management System (ULMS) \cite{bodenreiderUnifiedMedicalLanguage2004} using journal abstracts from MEDLINE as well as with clinical patient records. They then used these embeddings to compare predicted word similarity against human-judgements. Minarro-Gimenez et al \shortcite{minarro-gimenezExploringApplicationDeep2014} trained embeddings using medical manuals, articles, and Wikipedia articles, comparing predicted vector similarity between medications against the National Drug File - Reference Terminology (NDF-RT) ontology. Choi et al \cite{choiLearningLowDimensionalRepresentations2016} improved on this work by learning on large-scale health record data consisting of raw text from clinical notes mapped to concepts from UMLS. In their yet unpublished work, Beam et al \cite{beamClinicalConceptEmbeddings2018} use an ``extremely large'' database of clinical notes, insurance claims, and full journal texts, and develop a new system termed ``cui2vec'', mapping concepts into a set of unique identifiers based on UMLS, and then training vectors for these identifiers based on the occurrences of other identifiers within a certain window length. 

All of the above examples were both trained and evaluated on general medical data, from all fields of medicine. It is unclear how these models perform in specific fields of medicine. For example, we can consider the medical speciality of psychiatry, the field of medicine concerned with mental illness such as depression or schizophrenia. Prior work has shown that psychiatric symptoms are often described in a long, varied, and subjective manner \cite{forbushSittingPinsNeedles2013} which may present a particular challenge for NLP.

Prior work has explored whether domain adaptation (DA), techniques to adapt data from other domains to work on a target, can improve performance when applied to this sub-domain of psychiatry. Lee et al \cite{leeLeveragingExistingCorpora2018} used these techniques to improve the task of de-identifying psychiatric notes. Zhang et al \cite{zhangAdaptingWordEmbeddings2018} then applied DA to word embeddings trained from general language and medical sources, showing some improvements when targeting a psychiatric dataset. 


%CHANGE ABOVE TO MORE GENERAL, BUT CAN STILL USE THOSE SPECIFIC EXAMPLES. 

\subsection{!Contribution}

%This project aims to advance the application of word embedding techniques in psychiatry. Specifically, we will seek to  determine whether embeddings trained on general medical data perform as well on psychiatric content as they do on other domains within medicine. We are unaware of prior work investigating this. We will compare multiple techniques for embeddings and evaluation. This will help determine generally how well these performance on psychiatric concepts, and whether various attributes may help or hinder this applicability, such as embeddings trained on larger training sets, or the use of DA.  This may impact future work by suggesting if psychiatric applications should use general-medicine trained embeddings, or those trained only on domain-specific data.  


In this work, I seek to start understanding how NLP performance may vary when applied to the difference fields of medicine. Specifically, I compare the quality of embeddings trained on general medical data by the field of medicine they are related to, using a variety of metrics previously described in the literature. As NLP is applied to medicine, field-specific applications will become increasingly popular. If this work finds little difference between the fields, future will be assured that embeddings trained from general medical text will be sufficient. Conversely, if differences are found for specific fields, future work may want to address this shortfall by using techniques like DA, or even creating embeddings specifically trained for this field. 


\section{Proposed Methodology}

%\begin{itemize}
%	\item De Vine et al's \shortcite{devineMedicalSemanticSimilarity2014} embeddings trained on medical records and abstracts.  
%	\item Minarro-Gimenez et al's \shortcite{minarro-gimenezExploringApplicationDeep2014} embeddings trained on medical manuals and articles, Wikipedia.
%	\item Choi et al's \shortcite{choiLearningLowDimensionalRepresentations2016}'s two sets of embeddings trained differently using raw data mapped to a matrix based on UMLS techniques.  
%	\item Zhang et al's \shortcite{zhangAdaptingWordEmbeddings2018} best performing embeddings using domain-adaptation techniques. 
%	\item Beam et al's cui2vec embeddings trained on health insurance claims and full journal texts. 
%\end{itemize}

%The evaluation techniques to be replicated and used to determine psychatry-specific performance:
%\begin{itemize}
%	\item De Vine et al's \shortcite{devineMedicalSemanticSimilarity2014}'s evaluation framework, comparing predicted vector similarity against human judgements, using the evaluation from \cite{koopmanEvaluationCorpusdrivenMeasures2012} which compares predicted similarity against human judgements from \cite{pedersenMeasuresSemanticSimilarity2007} and \cite{caviedesDevelopmentConceptualDistance2004}.
%	\subitem I remember seeing these, where did we put them? Enough psych to matter?
%	\item Comparison against the UMNSRS benchmark per \cite{yuRetrofittingConceptVector2017}.
%	\subitem About 500 relations. However, include drugs, disorders, symptoms. Could do a cui2icd? Or hand annotate? Downloaded the file into data. Maybe use 'may treat into icd9's to figure out what system'? Or perhaps symptoms are related as well
%	\item Minarro-Gimenez et al's \shortcite{minarro-gimenezExploringApplicationDeep2014}'s metric of predicting relationships between drugs based on the NDF-RT. 
%	\subitem Figure out if this is different enough from the first Choi one 
%	\item Choi et al's \shortcite{choiLearningLowDimensionalRepresentations2016} Conceptual Similarity Property, comparing predicted vector similarity with whether concepts are neighbouring in UMLS. 
%	\subitem Look at more; weird performance and unsure how to consider vs others
%	\item Choi et al's \shortcite{choiLearningLowDimensionalRepresentations2016} Medical Relatedness Property, comparing predicted vector similarity with relatedness according to NDF-RT and the ICD9 groupings, based on these database's item relations such as ``may-treat'' and ``may-prevent''. 
%	\item Done, still look at top 10 maybe
%	\item Beam et al's \shortcite{beamClinicalConceptEmbeddings2018} statistical score based on whether known similarities in UMLS, NDF-RT and other work are predicted correctly in at least 95\% of bootstrapped samples of pairs of concepts. 
%	\subitem No code yet, sigh. Work on others first. Will need way to systemize drugs, non-disorders.
%\end{itemize}

In order to determine which psychiatric and non-psychiatric terms should be compared, the most common concepts shall be used. For instance, we will compare the most commonly prescribed psychiatric and non-psychiatric drugs, or the most common diagnoses, based on prior epidemiology, in order to compare common, well described concepts.

For top diagnoses: Could access fancy Canadian Data with a data request per emails from librarians. Or, can use top diagonses cards from ACP from ICD 10 \href{https://www.acponline.org/system/files/documents/running_practice/payment_coding/coding/icd10_coding_card.pdf}{here} or ICD 9 version which seems to skip mental disorders. Or could just do a "top 10" and show quality for all of those.


\section{Methods}

\subsection{Obtaining Embeddings}

\begin{table*}[h!]
	\begin{center}
		\caption{Charectoristics of the embeddings compared, including the name referred, the embedding dimensions, the number of embeddings in the dataset, and the type of data used to train them.}
		\label{tab:embed}
		\begin{tabular}{l|c|c|l} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
			\textbf{Name} & \textbf{Dimension} & \textbf{Number} & \textbf{Data Used to Train} \\
			\hline
			DeVine200 & 200 & 52,102 & clinical narratives\\
			&&&journal abstracts\\
			ChoiClaims300 & 300 & 14,852& health insurance claims\\
			ChoiClinical300 & 300 & 22,705& clinical narratives\\
			BeamCui2Vec500 & 500 & 109,053& health insurance claims\\
			&&&full journal text\\
		\end{tabular}
	\end{center}
\end{table*}

Table \ref{tab:embed} contains details of the embeddings compared in this project, all of which are based on \emph{word2vec}. We obtained DeVine200 \cite{devineMedicalSemanticSimilarity2014}, ChoiClaims300, and ChoiClinical300 \cite{choiLearningLowDimensionalRepresentations2016} all from the \href{https://github.com/clinicalml/embeddings}{later's website}. We downloaded BeamCui2Vec \cite{beamClinicalConceptEmbeddings2018} from \href{https://figshare.com/s/00d69861786cd0156d81}{this site}. 

% CAN INCLUDE BELOW IF YOU WANT
%The corresponding author for the remaining embeddings from Minarro-Gimenez and Zhang were contacted, but a response was not received. Miarro-Gimenez's project files are available at \href{https://code.google.com/archive/p/biomedical-text-exploring-tools/}{this code archive site} but lack documentation and could not be accessed by this time. 

%TODO: Try to extract the embeddings. Probably accessed by TestClientThreadWord2vec.java, try opening it up in Eclipse, maybe can modify it and retrieve them. 

\subsection{Evaluation Methods}

\paragraph{Determining a Concept Embedding's Field of Medicine}
A clinical concept could be understood to be a part of different medical fields depending on who is asked. In order to have an objective and unambiguous classification, we utilized the ninth revision of the International Statistical Classification of Diseases and Related Health Problems (ICD-9). This is a widely used system to classify medical diseases and disorders, and classifies such into seventeen chapters representing medical systems or other such clusters, such as mental disorders, or disease of the respiratory system.    

\paragraph{Medical Relatedness Property}(MRP)

This metric from \cite{choiLearningLowDimensionalRepresentations2016} is based on quantifying whether concepts with known relations are located near each other. They use, separately, known relationships between drugs and the diseases they treat or prevent, and the relations between diseases that are group together in the CCS hierarchical groupings, a classification from the Agency for Healthcare Research and Quality. The scoring utilizes Discounted Cumulative Gain, which attributes a diminishing score if a relationship is found within \emph{k} neighbours the further it is. 

In our implantation, we calculate the MRP based on the `course' groupings from CCS drug hierarchies. Scores are calculated for CUIs that represent diseases with a known ICD9 code. The mean MRP is then calculated for all CUIs within a given ICD9 system. 

DISCUSS THIS WAS IMPLEMNTED AND OTHERS FROM SCRATCH.

\paragraph{Medical Conceptual Similarity Property}(MCSP) The other metric from Choi et al's work evaluates whether embeddings known to be of a particular set are clustered together. They use conceptual sets from the UMLS such as `pharmacologic substance' or 'disease or syndrome'.  Discounted cumulative gain is again used, based on whether a cui has other cuis of its set within \emph{k} neighbours. 

We reimplement this method, but instead of using the UMLS conceptual sets, we create sets from the ICD9 systems. We consider CUIs that represent diseases with ICD9 codes, and CUIs that represent drugs which treat or prevent ICD9 diseases, and attribute score if drugs or diseases are near others of the same medical system. 

\paragraph{Correlation with UMNSRS Similarity}(SimCor)
\cite{yuRetrofittingConceptVector2017} investigate whether the cosine similarity of embedding vectors are correlated with human judgements of similarity. The UMNSRS-Similarity dataset \cite{pakhomovSemanticRelatednessSimilarity2018} contains around 500 similarity ratings between medical concepts as rated by eight medical residents. A Spearman rank correlation is then computer between the cosine similarities and the UMNSRS-Similarity ratings for pairs. 

Our implementation repeats the above. A medical system's mean correlation is calculated from all pairs that contain at least one disease with an ICD9 code in its system, or a drug that treats or prevents a disease in the system.  Soearman rank correlation is again used. 

\paragraph{Significance in Bootstrap Distribution}(Bootstrap)
Beam et al \shortcite{beamClinicalConceptEmbeddings2018} also evaluate how well known relationships between concepts are observed between embeddings, such as whether diseases are co-morbid, or a drug treats a condition. For a given type of relation, they generate a bootstrap distribution by randomly calculating cosine similarities of embedding vectors of the same class (eg. a random drug and disease when evaluating drug may-treat disease relations). For a given known relation, they consider the embeddings producing an accurate prediction if their cosine similarity is within the top 5\%, the equivalent of p<0.05 for a one-sided t-test. 

Our implementation considers the \emph{may-treat} or \emph{may-prevent} known relationships from the NDF-RT dataset. The percentage of known relations for drug-disease pair within each medical system is calculated. 

\paragraph{Centroid Prediction}
We implement a new method to compare against the others. A centroid is calculated for each medical system by averaging the normed embeddings for diseases with relevant ICD9 codes. We then calculate the percentage of drugs known to treat or prevent a disease in each system whose vectors are most similar (by cosine similarity) to the relevant centroid. For example, we would expect the CUI for `fluoxetine', an anti-depressant, to be most similar to the Mental Disorders centroid. 

Some drugs treat or prevent diseases in \emph{n} multiple system. For a medical system, such a drug is considered being accurately predicted if it the system's centroid is amongst the \emph{n} most similar centroids.  

\paragraph{Analysis}
Our work seeks to determine if embeddings for one medical system are worse or better than others. Additionally, we week to determine whether there are similar differences between the different sets of embeddings. To do this, we must consider the scores generated using five different metrics, four different embeddings, and seventeen different medical systems. However, the scores from five metrics are not obviously convertible, at least not all together. 

For now, we will assume our results are normally distributed; this may not be unreasonable as the processes underlying the quality of embeddings - the use of words representing clinical concepts in texts - stem from a natural process (human writing word choice).

To compare the embeddings from the different medical systems, for each evaluation method we calculate the mean score from each embedding. We then conducted a paired two-tailed t-test for five pairs of system's score with a given embedding vs the mean score across all systems for an embedding. We then observe the relative difference vs the mean, and whether this is significant at p \textless 0.05. 

We repeat the same steps to compare the sets of embeddings. This time, we calculate the mean scores from each medical system for each evaluation method, and conduct the paired two-tailed t-tests on seventeen pairs for each medical system. The pairs are the mean score from all the embedding sets, against the score from the given embedding set. Again, we calculate the score relative to mean, and significance as above. 

After observing that the embeddings from Beam et al well outperform the other embedding sets, we then evaluate the embeddings by medical system one last time using only MCSP. This is chosen as it resulted in the highest number of significant differences, and is able to calculate scores for all systems. Choosing one embedding set allows more embeddings to be compared, as scores can only be calculated for each method on embeddings shared by all embedding sets. Also choosing one evaluation method then allows statistical analysis to be performed on the individual embedding scores. As such, for each medical system we then perform a one-sided two-tailed t-test between its embeddings and the scores from all embeddings. We then repeated the same but only for the embeddings representing CUIs that were overlapping between the four embedding sets so that the effect of this restriction could be determined. 

%TODO MORE



\section{Results}

Comparing the embeddings from medical systems across the five evaluation methods reveals that some systems have scores significantly above the mean across multiple methods (Table \ref{tab:allsystemresults}). Embeddings related to cancers and muskoskeletal system are significantly above the mean in two methods, while those of mental disorders, the nervous system, and the cardiovascular system are significantly above in three methods. No systems have scores below the mean on more than one metric; those that do include those related to diseases of pregnancy, the perinatal period, and skin disorders. 

\begin{table*}[]
	\caption{Score of embeddings from a given medical system according to a given evaluation method expressed as relative to the mean score for that method. Significant (paired t-test p \textless 0.05) scores above are shown in orange, below blue. See Methods section for method abbreviations. Blank values represent no scores could be calculated for a given combo.}
	\label{tab:allsystemresults}
	\begin{tabular}{lccccc}
		& MRP                           & MCSP                          & SimCor                        & Bootstrap                    & Centroid                     \\
		Infections           & -0.206                        & 0.408                         & \blu{-0.341} & \ora{0.348} & 0.119                        \\
		Cancers              & \ora{0.194}  & 0.019                         & \ora{0.514}  & 0.382                        & -0.04                        \\
		Cndocrine            & 0.029                         & \blu{-0.116} & \ora{0.199}  & 0.229                        & -0.1                         \\
		Blood diseases       & -0.071                        & \blu{-0.285} & -0.02                         & -0.063                       & 0.097                        \\
		Mental disorders     & \ora{0.23}   & \ora{0.666}  & -0.449                        & 0.25                         & \ora{0.325} \\
		Nervous system       & \ora{0.61}   & \ora{0.638}  & -0.028                        & 0.174                        & \ora{0.406} \\
		Cardiovascular       & \ora{0.117}  & \ora{0.641}  & 0.189                         & 0.218                        & \ora{0.384} \\
		Respiratory          & -0.13                         & 0.066                         & \blu{-0.242} & \ora{0.402} & -0.07                        \\
		Digestive            & 0.239                         & -0.072                        & \blu{-0.421} & -0.067                       & -0.385                       \\
		Genitourinary        & 0.285                         & 0.08                          & -0.149                        & 0.073                        & \ora{0.178} \\
		Pregnancy            & -0.219                        & \blu{-0.56}  &                               &                              &                              \\
		Skin                 & \blu{-0.276} & -0.203                        & -0.312                        & 0.18                         & 0.075                        \\
		Muskoskeletal        & 0.046                         & 0.13                          & \ora{0.467}  & 0.064                        & \ora{0.305} \\
		Congenital           & -0.15                         & -0.128                        &                               & 0.106                        & \ora{0.524} \\
		Perinatal            & -0.382                        & \blu{-0.567} &                               &                              &                              \\
		Ill-defined          & -0.176                        & -0.281                        & -0.244                        & -0.183                       & 0.053                        \\
		Injury and poisoning & -0.138                        & \blu{-0.437} & \ora{0.835}  & -0.115                       & 0.128                       
	\end{tabular}
\end{table*}

 Evaluating the sets of embeddings (Table \ref{tab:allsystemresults}) shows some stark differences. The \emph{cui2vec} embeddings from Beam et al are above the mean across all evaluation methods, significantly three times. Those from DeVine et al, and those from Choi et al based on the clinical narratives, do significantly worse. The remaining embeddings, those based on health insurance codes from Choi et al, are more middling. 


\begin{table*}[]
	\caption{Score of set of embeddings according to a given evaluation method expressed as relative to the mean score for that method. Significant (paired t-test p \textless 0.05) scores above are shown in orange, below blue. See Methods section for embedding set abbreviations.}
	\label{tab:allembedresults}
	\begin{tabular}{llllll}
			            &MRP	        &MCSP	           &SimCor	    &Bootsrap	  &Centroid\\
		DeVine200       &\blu{-0.345}   &\blu{-0.121}	   &0.065	    &\blu{-0.464} &	-0.042 \\
		ChoiClaims300   &\ora{0.097}	&0.036	           &-0.055	    &-0.041	      &0.018   \\
		ChoiClinical300	&\blu{-0.135}	&\blu{-0.155}	   &-0.047	    &-0.019	      &-0.005  \\
		BeamCui2Vec500	&\ora{0.384}	&\ora{0.24}	       &0.037       &\ora{0.524}  &0.029   \\
	\end{tabular}
\end{table*}

In Table \ref{tab:onlybeamonlymcsp} we focus on MCSP scores from the best performing set of embeddings, those from Beam et al. We observe results that are somewhat similiar to the joint analysis. Embeddings related to mental disorders, the nervous system, and the cardiovascular system again perform when all relevant embeddings are observed, in line with the general results. When only the overlapping embeddings are considered, muskoskeletal embeddings are below average, while genitourinary are above, though the other three are again better. Of note, these results do become weight on the number of examples found per system, unlike prior results. While these results descriptively seem to match, the mean MCSPs are not well correlated between those using all embeddings vs only the overlapping ones. Assuming normality, Pearson's coefficent is only 0.16, while not assuming this and using Spearman's we get only 0.01. 


\begin{table*}[]
	\caption{Comparison of MCSP scores using the embeddings from Beam et al when considering all relevant embeddings, and only those that are overlapping with the other sets of embeddings. The MCSP column repersents the mean MCSP score, Examples the number of embeddings per medical system, Relative the relative performance against the mean for all systems, orange/blue if significantly above/below at p \textless 0.05. The final column is whether the scores for a system are significantly different all embeddings vs overlapping.}
	\label{tab:onlybeamonlymcsp}
	\begin{tabular}{llllllll}
		& \multicolumn{3}{l}{All Relevent Embeddings} & \multicolumn{3}{l}{Overlapping Embeddings} &            \\
		ICD9 Systems         & MCSP      & Examples     & Relative         & MCSP     & Examples     & Relative         & Different? \\
		Infections           & 7.72      & 2261         & \blu{-0.043}     & 6.84     & 334          & 0.223            & Yes        \\
		cancers              & 9.00      & 1194         & 0.116            & 4.47     & 116          & \blu{-0.202}     & Yes        \\
		endocrine            & 5.64      & 545          & \blu{-0.301}     & 3.98     & 193          & \blu{-0.29}      & Yes        \\
		blood diseases       & 4.36      & 199          & \blu{-0.459}     & 3.67     & 81           & \blu{-0.345}     & Yes        \\
		mental disorders     & 9.34      & 662          & 0.158            & 7.67     & 165          & 0.371            & Yes        \\
		nervous system       & 8.44      & 1787         & \ora{0.046}      & 7.27     & 434          & 0.298            & Yes        \\
		cardiovascular       & 8.12      & 869          & \ora{0.007}      & 7.74     & 307          & 0.383            & No         \\
		respiratory          & 5.85      & 405          & \blu{-0.274}     & 4.64     & 132          & \blu{-0.17}      & Yes        \\
		digestive            & 7.93      & 852          & \blu{-0.016}     & 4.62     & 210          & \blu{-0.175}     & Yes        \\
		genitourinary        & 6.82      & 606          & \blu{-0.154}     & 5.75     & 210          & \ora{0.027}      & Yes        \\
		pregnancy            & 10.27     & 1325         & 0.273            & 2.47     & 10           & \blu{-0.559}     & Yes        \\
		skin                 & 5.10      & 305          & \blu{-0.368}     & 4.01     & 102          & \blu{-0.283}     & Yes        \\
		muskoskeletal        & 8.22      & 1041         & \ora{0.019}      & 5.24     & 168          & \blu{-0.064}     & Yes        \\
		congenital           & 6.24      & 457          & \blu{-0.227}     & 5.05     & 101          & \blu{-0.098}     & Yes        \\
		perinatal            & 9.84      & 310          & 0.22             & 2.49     & 11           & \blu{-0.555}     & Yes        \\
		ill-defined          & 2.68      & 558          & \blu{-0.668}     & 2.81     & 224          & \blu{-0.498}     & No         \\
		injury and poisoning & 9.09      & 2975         & 0.127            & 2.42     & 86           & \blu{-0.569}     & Yes       
	\end{tabular}
\end{table*}

\section{Discussion}

\subsection{Embedding Quality by Field of Medicine}
In this project, we seek to determine whether embeddings for clinical concepts learned from general medical text well well for the various fields of medicine. We investigate this by using different metrics of embedding quality and different sets of embeddings, and use ICD9 systems to determine the relevant medical fields. Based on the methods used so far, our results suggest a consistent pattern the embeddings from certain medical systems perform better than others- namely, those of mental disorders, the nervous system, and those of the cardiovascular system, and possibly the muskoskeletal system. It is less clear if any systems perform particularly poorly, though some of the systems had few or no relevant embeddings to for some metrics. 

Why some of these systems seem to perform better are unclear. If we take the numbers of compared embeddings as a proxy for frequency, the well performing systems do not stand out as either popular nor unpopular. The frequency of concepts found in the training data may be a cause, and presents an option to examine quantitatively in the future.

Strengths of the project include an objective means to relate a clinical concept to a field of medicine, and the combination of multiple evaluation metrics and sets of embeddings to well survey relevant work. However, these also present weakness. First and foremost, the combination of metrics and embeddings and systems creates a daunting task for statistical analysis, and this could likely be improved by considering advanced techniques such as non-parametric equivalents to ANOVA and ensuring post-hoc analysis. 

Additionally, we see that there is a difference in the results when fewer or greater numbers of embeddings are considered. The initial analysis only considered embeddings that were common to all embedding sets, significantly restrict the numbers of embeddings considered compared to what could be considered when only using the \emph{cui2vec} embeddings from Beam et al. This score difference may very well be due to whether less common medical terms are selected; the set of overlapping embeddings likely represent more common clinical concepts if all four training methods found them. 

This last people is important to consider when assessing the project's results. The differences we find between medical systems could be largely due to how many rare clinical concepts a system has; these rare terms may have had fewer chances to be trained well due to seldomly coming up in the training data. This may or may not be a confounder. A future researcher using the embeddings in a particular field of medicine may be using embeddings from a variety of concepts in her field, including rare ones, in which case worse performance due to a field containing rare terms would be desired. Conversely, applications may only consider common terms in a field, in which case these rare terms would indeed be confounding. This limitation can be addressed, as will be discussed below. 

\subsection{Evaluating Sets of Embeddings}

Another goal of the project was to asses the quality of embeddings from each set. This can be difficult to truly asses, as there is no gold standard to compare ratings. However, we believe the results indicated that the various evaluation methods used did do a good job of comparing qualities. Namely, Beam et al's \emph{cui2vec} embeddings clearly come out ahead. This is expected, as these embeddings are the most modern, are trained on the largest amount of data, are trained on both journal articles and insurance data, and contain the largest number of embeddings. Our main contrbution in this line is the use of ICD9 systems as a new source of known relationships/clusters, which we then implemented in some of the methods. We believe these relations may be more clinically relevant than some used in the past, such as whether a clinical concept is a drug or symptom, though this is up for future work to decide. 

ANY MORE IN THIS SECTION?

\subsection{Comparing Evaluation Methods}
Our work also allows us to observe differences between the evaluation metrics used. Again, this is difficult to concretely judge, as do we not know what the methods should result in. Yu et al's method of comparing correlation with similiarity as judged by resident physicians was hampered by having a small number of judged similarities (around 500) which led to few comparisons per system in our deployment. Our new method attempted, while simple, found smaller differences between both systems and sets of embeddings. Neither method finds the \emph{cui2vec} embeddings to be significantly better than the others. Taken together, this may suggest these two methods are inferior. However, our new method does find similar results to the other methods, for instance also finding the significantly better systems, so it may not be worse. The method from Yu et al could likely be slight improved; some of the human simiarlity comparisons are not used as the CUIs chosen are not found within our embeddings, but could be changed to very related ones that are, eg Diabetes --> Diabetes Mellitus.  

The remaining methods are quite similar. The two methods from Choi et al use discounted cumulative gain to judge whether neighbouring vectors are part of known relations, while Beam et al's method evaluates these relationships compared to a bootstrapped sample. The three methods could all use different sets of relation metrics, further muddling their relative performance. For instance, Beam et al's method as implement in our project evaluates known relations between drugs and the diseases they treat or prevent, but it could instead be changed to evaluate whether two concepts are related to the same medical system. This represents yet another degree of freedom inherent in our results, which challenges the interpretability. THIS IS KINDA WEIRD. 

\subsection{!Lessons Learned}
The main lesson I learned from this project was to consider the evaluation ahead of time. The many variables - which set of embeddings, which system, which evaluation method, what the evaluation method uses for comparisons - make it a challenge to sort out definitive results. However, I am not sure what I would've changed retrospectively, and much of this lesson may simply be ``time to learn some more stats''. 

\subsection{!Was Project Succesful?}
\subsection{!Strengths and weaknesses}

\section{!Future Work}
\paragraph{Improving the current project}: Multiple avenues exist for improving the current project. Multiple methods exist for further statistical analysis, including those that are non-parametric, and those that would be able to consider all raw scores, without needing to work on means and disregard some data such as counts. 

Currently, the dictionary between ULMS CUIs and ICD9CM codes only has around 40,000 entries. As such, many CUIs could not be used, despite representing concepts very related. Generating a larger dictionary automatically would be useful, and could possible be feasible given the requirement that they only be fit into large disease categories. 

Further consideration of the combination of variables could also be helpful. For instance, all of the evaluation methods could be adapted to use as their known-relation whether an embedding's CUI is within a system; this could reduce as a variable. 

As discussed previously, a possible confounded is whether embeddings from a given medical system performance worse due to containing rare diseases. While not necessarily an unintended feature, this could be ameliorated by only using common CUIs. This was intended to be a part of this current project, but was deferred. Frequencies of ICD9 codes are not readily available except within public health databases which requires formal applications for, which likely could not be acquired in the timeframe of this project, though they represent an interesting avenue for future work. 
 
\paragraph{Possible extension}

Evaluating Zhang et al's domain adaptation-trained embeddings in this project would help quantify how much improvement this technique may lead, if their embeddings could be obtained. DA could be carried out on embeddings from a poorly performing medical system to see if it is improved. Or, for a larger project, medical-field-specific embeddings could be trained, and compared to the ones used in this project, in order to determine the scale of possible improvement. Finally, our methods for evaluation consider metrics for embedding quality. While a basic component of possible NLP applications, a more `real-world' evaluation could be attempted carrying out actual NLP tasks on documents from different medical systems to understand their relative performance from this perspective. For instance, NLP tasks using the embeddings could be carried out on articles on conditions from the various medical systems from UpToDate, a medical encyclopaedia, or Wikipedia. Or, even more applied, NLP tasks could be evaluated on medical documents produced by physicians of different specialities, an opportunity I may soon have with a personal project. 



%If the proposed methodology is implemented easily and quickly, a possible extension will be determine the feasibility of training new embeddings based only on psychiatric data, such as using a subset of the matrix used by Choi et al's \shortcite{choiLearningLowDimensionalRepresentations2016}; we could try only using the portion of the matrix with terms related to psychiatry. 
%
%Alternatively, it may be interesting to use the embeddings from prior work to carry out various document-level summarization techniques, and compare doing so for psychiatric vs non-psychiatric documents. For instance, this could be done on articles from Wikipedia describing popular illnesses in and outside of psychiatry, or a similar set of articles from the medical practice manual and learning resource UpToDate.
%
%In the longer term, this project may be applicable to a separate project applying NLP and ML techniques to a large BC Cancer clinical dataset consistency of the medical records of around 50,000 patients and their free text medical documents, numbering in the 100,000's. This dataset may allow both evaluation or training when available in the future. 


\bibliographystyle{acl}
\bibliography{my_library}{}

%
%\begin{thebibliography}{}
%	
%
%	
%
%\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
%Alfred~V. Aho and Jeffrey~D. Ullman.
%\newblock 1972.
%\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
%\newblock Prentice-{Hall}, Englewood Cliffs, NJ.
%
%\bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
%{American Psychological Association}.
%\newblock 1983.
%\newblock {\em Publications Manual}.
%\newblock American Psychological Association, Washington, DC.
%
%\bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
%{Association for Computing Machinery}.
%\newblock 1983.
%\newblock {\em Computing Reviews}, 24(11):503--512.
%
%\bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
%Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
%\newblock 1981.
%\newblock Alternation.
%\newblock {\em Journal of the Association for Computing Machinery},
%  28(1):114--133.
%
%\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
%Dan Gusfield.
%\newblock 1997.
%\newblock {\em Algorithms on Strings, Trees and Sequences}.
%\newblock Cambridge University Press, Cambridge, UK.
%
%\end{thebibliography}

\end{document}
