\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp-ijcnlp-2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}
%\usepackage{hltnaacl04}
%\usepackage{hyperref}
\usepackage{pgfplotstable}
\usepackage{csvsimple}
\usepackage{pgfplotstable}
\usepackage{color}
\usepackage{boldline}
%\usepackage{listings}

\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\textbf{\color{blu}#1}}}

\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}

\definecolor{ora}{rgb}{1,0.6,0}
\def\ora#1{{\textbf{\color{ora}#1}}}


\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Comparing the Intrinsic Performance of Clinical Concept Embeddings by Their Field of Medicine}

\author{John-Jose Nunez\\
  Depts. of Psychiatry and Computer Science, UBC\\
  {\tt jjnunez@cs.ubc.ca} 
}
\date{}

\begin{document}
\maketitle
\begin{abstract}
	Pre-trained word embeddings are becoming increasingly popular for natural language-processing tasks. This includes medical applications, where embeddings are trained for clinical concepts using specific medical data. Recent work continues to improve on these embeddings. However, no one has yet sought to determine whether these embeddings work as well for one field of medicine as they do in others. In this work, we use intrinsic methods to evaluate embeddings from the various fields of medicine as defined by their ICD-9 systems. We find significant differences between fields, and motivate future work to investigate whether extrinsic tasks will follow a similar pattern. 
\end{abstract}

\section{Introduction}

%%6-7 pages including max 1 page references

%%DESCRIPTION OF PROBLEM EG NLP TASK, CORPUS
%%RELATED WORK
The application of natural language processing (NLP) and machine learning to medicine presents an exciting opportunity for tasks requiring prediction and classification. Examples so far include predicting the risk of suicide or accidental death after a patient is discharged from general hospitals ~\cite{mccoyImprovingPredictionSuicide2016} or classifying which patients have peripheral vascular disease ~\cite{afzalMiningPeripheralArterial2017}. A common resource across NLP for such tasks is to use high-dimensional vector word representations. These word embedding include the popular \emph{word2vec} system \cite{mikolovEfficientEstimationWord2013} which was initially trained on general English text, using a skip-gram model on a Google News corpus.

Due to considerable differences between the language of medical text and general English writing, prior work has trained medical embeddings using specific medical sources. Generally, these approaches have trained embeddings to represent medical concepts according to their `clinical unique identifiers' (CUIs) in the Unified Library Management System (ULMS) \cite{bodenreiderUnifiedMedicalLanguage2004}. Words in a text can then be mapped to these CUIs \cite{yuShortIntroductionNILE2013}. Various sources have been used, such as medical journal articles, clinical patient records, and insurance claims \cite{devineMedicalSemanticSimilarity2014}, \cite{minarro-gimenezExploringApplicationDeep2014},  \cite{choiLearningLowDimensionalRepresentations2016}.  

Prior authors have sought to improve the quality of these embeddings, such as using different training techniques or more training data \cite{beamClinicalConceptEmbeddings2018}. In order to judge the quality of these embeddings, they have primarily used evaluation methods quantifying intrinsic qualities, such as their ability to predict drug-disease relations noted in the National Drug File - Reference Terminology (NDF-RT) ontology \cite{minarro-gimenezExploringApplicationDeep2014}, or whether similar types of clinical concepts had cosine similiar vectors  \cite{choiLearningLowDimensionalRepresentations2016}.

To date these embeddings have been both trained and evaluated on general medical data. That is, no fields of medicine were specified or excluded; data could be from an obstetrician delivering a baby, a cardiologist placing a stent, or a dermatologist suggesting acne treatment. It is unclear how well such embeddings perform for a specific field of medicine. For example, we can consider psychiatry, the field of medicine concerned with mental illnesses such as depression or schizophrenia. Prior work has shown that psychiatric symptoms are often described in a long, varied, and subjective manner \cite{forbushSittingPinsNeedles2013} which may present a particular challenge for training these embeddings and NLP tasks generally.

As these pre-trained embeddings may increasingly be used for down-stream NLP tasks in specific fields of medicine, we seek to determine whether embeddings from one field perform relatively well or poorly relative to others. Specifically, we aim to follow prior work using intrinsic evaluation methods, comparing the geometric properties of embedding vectors against others given known relationships. This will offer a foundation for future work that may compare the performance on extrinsic NLP tasks in different medical fields. Finding relative differences may support that certain medical fields would benefit from embeddings trained on data specific to their field, or using domain adaptation techniques as sometimes used in the past \cite{yuRetrofittingConceptVector2017}. 


%
%
%Clinical concept embeddings to-These medical embeddings 
%
%Examples include De Vine et al  who trained embeddings using journal abstracts from MEDLINE as well as with clinical patient records. They evaluated these embeddings by comparing vector cosine similarity against human-judged similarity. Minarro-Gimenez et al \shortcite{minarro-gimenezExploringApplicationDeep2014} trained embeddings using medical manuals, articles, and Wikipedia articles, judging quality by their ability to predict drug-disease relations noted in the National Drug File - Reference Terminology (NDF-RT) ontology. Choi et al \cite{choiLearningLowDimensionalRepresentations2016} built on this work by learning two sets of embeddings, from health insurance claims and clinical narratives. They evaluated their embeddings by their ability to predict known relations including those in the NDF-RT, disease hierarchies, and medical concept type. In their yet unpublished work, Beam et al \cite{beamClinicalConceptEmbeddings2018} learn embeddings on the largest dataset yet, combining health insurance claims, clinical narratives and full journal texts. They developed a new system termed ``cui2vec'', training CUI embeddings based on the occurrences of other identifiers within a certain window length. They use an assortment of aforementioned known relations to compare the quality of these embeddings. 
%
%All of the above examples were both trained and evaluated on general medical data, from all fields of medicine. It is unclear how these embeddings perform for a specific fields of medicine. For example, we can consider the medical speciality of psychiatry, the field of medicine concerned with mental illness such as depression or schizophrenia. Prior work has shown that psychiatric symptoms are often described in a long, varied, and subjective manner \cite{forbushSittingPinsNeedles2013} which may present a particular challenge for training these embeddings and NLP tasks generally.
%
%Prior work has explored whether domain adaptation (DA), techniques to adapt data from other domains to work on a target, can improve performance when applied to this sub-domain of psychiatry. Lee et al \cite{leeLeveragingExistingCorpora2018} used these techniques to improve the task of de-identifying psychiatric notes. Zhang et al \cite{zhangAdaptingWordEmbeddings2018} then applied DA to word embeddings trained from general language and medical sources, showing some improvements when targeting a psychiatric dataset. 
%
%As these pre-trained embeddings may be increasingly be used for down-stream NLP tasks in specific fields of medicine, we seek to determine whether embeddings work as well for one field as they do in others. Specifically, we aim to follow prior work and do this using intrinsic evaluation methods, which compare the geometric properties of embeddings vectors against others given known relationships. This will offer a foundation for future work that may compare the performance of extrinsic NLP tasks in different medical fields. The differences found will support whether future work may benefit from training embeddings on data from specific medical fields, or using techniques such as domain adaptation. 
%

%
%If this work finds little difference between fields of medicine, future researchers will be assured that embeddings trained from general medical text will be sufficient. Conversely, if differences are found for specific fields, future work may want to address this shortfall by using techniques like DA, or even creating embeddings specifically trained for specific fields. 
%
%Additionally, this project also contributes to the evaluation of medical embeddings. Existing methods generally evaluate whether the embedding vectors for a given concept are similar to other embeddings given a known relation. This work is the first to use a concept's field of medicine as such a relation, which we believe may be more clinically relevant than some used previously. 
%
%Lastly, by comparing multiple sets of embeddings using different evaluation metrics, this work seeks to also evaluate the relative performance of embeddings trained with different methods. 




%This project aims to advance the application of word embedding techniques in psychiatry. Specifically, we will seek to  determine whether embeddings trained on general medical data perform as well on psychiatric content as they do on other domains within medicine. We are unaware of prior work investigating this. We will compare multiple techniques for embeddings and evaluation. This will help determine generally how well these performance on psychiatric concepts, and whether various attributes may help or hinder this applicability, such as embeddings trained on larger training sets, or the use of DA.  This may impact future work by suggesting if psychiatric applications should use general-medicine trained embeddings, or those trained only on domain-specific data.  




% ONE DAY CONSIDER BELOW
%In order to determine which psychiatric and non-psychiatric terms should be compared, the most common concepts shall be used. For instance, we will compare the most commonly prescribed psychiatric and non-psychiatric drugs, or the most common diagnoses, based on prior epidemiology, in order to compare common, well described concepts.

%For top diagnoses: Could access fancy Canadian Data with a data request per emails from librarians. Or, can use top diagonses cards from ACP from ICD 10 \href{https://www.acponline.org/system/files/documents/running_practice/payment_coding/coding/icd10_coding_card.pdf}{here} or ICD 9 version which seems to skip mental disorders. Or could just do a "top 10" and show quality for all of those.


\section{Methods}

\subsection{Sets of Embeddings}

\begin{table*}[h!]
	
	\begin{center}
		\begin{tabular}{lcccl} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
			Name & Dimension & Number & Number of Training Data& Type of Training Data \\
			\hlineB{4}
			DeVine200 & 200 & 52,102 & 17k + 348k &clinical narratives, journal abstracts\\
			\hline
			ChoiClaims300 & 300 & 14,852& 4m&health insurance claims\\
			\hline
			ChoiClinical300 & 300 & 22,705&20m& clinical narratives\\
			\hline
			BeamCui2Vec500 & 500 & 109,053&60m + 20m + 1.7m& claims, narratives, full journal texts\\
		\end{tabular}
	\end{center}
	\caption{Characteristics of the embeddings compared, including the name referred, the embedding dimensions, the number of embeddings in the dataset, and the type of data used to train them.}
	\label{tab:embed}
\end{table*}

We sought to compare a variety of clinical concept embeddings trained on medical data. Table ~\ref{tab:embed} contains details of the sets compared in this project, all of which are based on \emph{word2vec}. We obtained DeVine200 \cite{devineMedicalSemanticSimilarity2014}, ChoiClaims300, and ChoiClinical300 \cite{choiLearningLowDimensionalRepresentations2016} all from the \href{https://github.com/clinicalml/embeddings}{latter's Github}. We downloaded BeamCui2Vec500 \cite{beamClinicalConceptEmbeddings2018} from \href{https://figshare.com/s/00d69861786cd0156d81}{this site}. Unfortunately, we were unable to obtain other sets of embeddings mentioned in the literature \cite{minarro-gimenezExploringApplicationDeep2014}, \cite{zhangAdaptingWordEmbeddings2018} \cite{xiangTimesensitiveClinicalConcept2019}.


\subsection{Determining a Field of Medicine's Clinical Concepts}
A clinical concept's corresponding field of medicine is not necessarily obvious. In order to have an objective and unambiguous classification, we utilized the ninth revision of the International Statistical Classification of Diseases and Related Health Problems (ICD-9) \cite{sleeInternationalClassificationDiseases1978}. This is a widely used system of classifying medical diseases and disorders, dividing them into seventeen chapters representing medical systems/categories such as mental disorders, or disease of the respiratory system. While the 10th version is available, we chose this version based on prior work using it, and the pending release of the 11th version. We will use these ICD9 systems to define the different medical fields. 

We determined a CUI's field of medicine according to a CUI-to-ICD9 dictionary available from the UMLS \cite{bodenreiderUnifiedMedicalLanguage2004}. We consider pharmacological substance related to a field of medicine system if it treats or prevents a disease with an ICD9 code within a particular ICD9 system. We determine this by using the NDF-RT dictionary, which maps CUIs of substances to the CUIs of conditions they treat or prevent, and then convert these CUIs to the ICD9 systems as before.  As such, A CUI representing a drug may have multiple ICD9 systems and therefore medical fields. 

\subsection{Evaluation Methods}

\begin{table*}[h!]
	\begin{center}
		\begin{tabular}{lcccl} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
			Drug & Actual Medical Field & Predicted Medical Field & Correct?\\
			\hlineB{4}
			Fluoxetine & Mental Disorders & Mental Disorders & Yes \\
			\hline
			Sertraline & Mental Disorders & Neoplasms & No \\
			\hline
			Risperidone & Mental Disorders & Mental Disorders & Yes \\
			\hline
			Olanzapine & Mental Disorders & Mental Disorders & Yes\\
			\hline
			Valproic Acid & \begin{tabular}[c]{@{}c@{}}Mental Disorders \\Diseases of the Nervous System \end{tabular} & \begin{tabular}[c]{@{}c@{}}Mental Disorders \\Congenital Abnormalities \end{tabular} & Yes\\
			\hline
			Lamotragine & \begin{tabular}[c]{@{}c@{}}Mental Disorders \\Diseases of the Nervous System \end{tabular} & \begin{tabular}[c]{@{}c@{}}Diseases of the Skin \\Diseases of the Nervous System \end{tabular} & No \\
			\hline
			&  & \textbf{Mental Disorders SysVec Score:}& \textbf{4/6 = 0.67}
		\end{tabular}
	\end{center}
	\caption{Illustrative example showing how System Vector Accuracy (SysVec) would be calculated for the medical field ``Mental Disorders" if it contained only six drugs. Predicted medical field is the medical field/ICD9 system vector closest to the drug, or $n$ closest fields if a drug treats conditions in $n$ multiple fields. System vectors are the normalized mean vector of that system's medical conditions.}
	\label{tab:sysvectoy}
\end{table*}

We sought to compare multiple methods for evaluating the quality of a medical field's embeddings based on prior work. We were unable to use Yu et al's \shortcite{yuRetrofittingConceptVector2017} method, based on comparing the correlation of vector cosine similarity against human judgements from the UMNSRS-Similarity dataset \cite{pakhomovSemanticRelatednessSimilarity2018} due to there being too few examples across many medical fields. 
The code for all implemented methods will be publicly available upon publication of this work from the  \href{https://github.com/ANONforREVIEW}{first author's GitHub}.

\paragraph{Medical Relatedness Measure (MRM)}

This method from Choi et al \shortcite{choiLearningLowDimensionalRepresentations2016} is based on quantifying whether concepts with known relations are neighbours of each other.  They use known relationships between drugs and the diseases they treat or prevent, and also the relations between diseases that are grouped together in the Clinical Classifications Software (CCS) hierarchical groupings, a classification from the Agency for Healthcare Research and Quality \cite{ClinicalClassificationsSoftware}. The scoring utilizes Discounted Cumulative Gain, which attributes a diminishing score the further away a known relationship is found if within \emph{k} neighbours. 

In our implementation, we calculate the Medical Relatedness Measure (MRM) based on the `coarse' groupings from the CCS hierarchies. Scores are calculated for CUIs that represent diseases with a known ICD9 code. The mean MRM is then calculated for all CUIs within a given ICD9 system. The implementation was adapted from Python 2.7 code available from the original author's \href{https://github.com/clinicalml/embeddings}{Github}. We calculate MRM as:

MRM${(V,F,k)=\frac{1}{|V(F)|}\sum\limits_{v\in V(F)}\frac{1}{|V(G)|}\sum\limits_{i=1}^k \frac{1_G(v(i))}{log_2(i+1)}}$

Where $V$ are medical conditions, $F$ a field of medicine, $V(F)$ the medical conditions within an ICD-9 system/field of medicine, $G$ the CCS group that medical condition $v\in V(F)$ is part of,  and $V(G)$ the subset of medical conditions found in this group. $1_G$ is 0 or 1 depending on whether $v(i)$, the $ith$ closest neighbour to a condition $v$, is in the same group. $k$ neighbours are considered. 

To illustrate this, consider calculating the MRM for $F$ ``Diseases of the Musculoskeletal System". It involves summing the scores for its conditions, such as \emph{rheumatoid arthritis ($v\in V(F)$)}. This condition is part of the CCS-coarse grouping ($G$), ``Rheumatoid arthritis and related disease". This group contains twelve conditions, such as \emph{Felty's syndrome} and \emph{Rheumatoid lung}. With Choi et al's choice of $k=40$, the score for \emph{rheumatoid arthritis} would depend on how many of the eleven other conditions in this group are within the 40 nearest neighbours ($v(i)$) to \emph{rheumatoid arthritis}, and would give a higher score the nearer they are, the highest being if they are the eleven nearest neighbours. 

\paragraph{Medical Conceptual Similarity Measure (MCSM)} The other method used by Choi et al's work evaluates whether embeddings known to be of a particular set are clustered together. They use conceptual sets from the UMLS such as `pharmacologic substance' or `disease or syndrome'.  Discounted Cumulative Gain is again used, based on whether a CUI has other CUIs of its set within \emph{k} neighbours. 

We reimplement this method, but instead of using the UMLS conceptual sets, we create sets from the ICD9 systems, again giving a score to neighbours that are diseases or drugs from the same field of medicine/ICD9 system. Again, this was adapted from code from Choi et al's Github. The Medical Conceptual Similarity Measure (MCSM) can be represented as:

MCSM$(V,F,k) = \frac{1}{|V(F)|}\sum\limits_{v\in V(F)}\sum\limits_{i=1}^k \frac{1_F(v(i))}{log_2(i+1)}$

Similar to MRM, $F$ is a medical field/ICD9 system, $V(F)$ the medical conditions within a system, and $1_F$ 0 or 1 depending on whether neighbour $v(i)$ is also in this medical field.

For illustration, consider an example calculating the MCSM for the medical field/system ($F$) ``Infectious and Parasitic Diseases". This involves calculating the score for the medical condition ($v$) \emph{primary tuberculous infection} . If \emph{rifampin}, an antibiotic, was found to be nearby, it would contribute to the MCSM, as it treats conditions in ``Infectious and Parasitic Diseases" and so would be classified as being part of this system. On the other hand, if the respiratory illness \emph{asthma} was one of the $k$ nearest neighbours, it would add nothing to the MCSM score, as it is a disease in a different system, ``Diseases of the Respiratory System". 

%\paragraph{Correlation with UMNSRS Similarity}(SimCor)
%\cite{yuRetrofittingConceptVector2017} investigate whether the cosine similarity of embedding vectors are correlated with human judgements of similarity.  contains around 500 similarity ratings between medical concepts as rated by eight medical residents. Yu et all then compute a Spearman rank correlation between the cosine similarities and the UMNSRS-Similarity ratings.
%
%We repeated the above, calculating a medical system's mean correlation based on the Spearman rank correlation of all pairing that contain at least one disease with an ICD9 code in its system, or a drug that treats or prevents a disease in the system. This was implemented from scratch in Python, and we used the \href{https://conservancy.umn.edu/handle/11299/196265}{UMN SRS modified similarity dataset}.

\paragraph{Significance against Bootstrap Distribution (Bootstrap)}
Beam et al \shortcite{beamClinicalConceptEmbeddings2018} also evaluate how well known relationships between concepts are represented by embedding vector similarity. For a given known relation, they generate a bootstrap distribution by randomly calculating cosine similarities between embedding vectors of the same class (eg. a random drug and disease when evaluating  drug-disease relations). For a given known relation, they consider that the embeddings produced an accurate prediction if their cosine similarity is within the top 5\%, the equivalent of $p<0.05$ for a one-sided t-test. 

Our implementation considers the may-treat or may-prevent known relationships from the NDF-RT dataset. We calculate the percentage of known relations for drug-disease pair within each medical field. Beam et al have not yet made their code publicly available, so we reimplemented this technique in Python. 

\paragraph{System Vector Accuracy (SysVec)}
We implement a new, simple method to evaluate a medical field's embeddings. A representative vector is calculated for each medical field/ICD9 system by taking the mean of the normalized embedding vectors of a field's diseases. We then consider all of the drugs known to treat or prevent a disease of a given medical field. A field's \emph{System Vector Accuracy} is then the percentage of these drugs whose most similar (by cosine similarity) system vector is this field's. A higher score indicates better performance. We implemented this method in Python. 

For example, a system vector for ``Mental Disorders" would be calculated from the embeddings for diseases such as \emph{schizophrenia} and \emph{major depressive disorder}. ``Mental Disorders'" \emph{System Vector Accuracy} is the percentage of its medications (e.g. \emph{fluoxetine}, \emph{risperidone}, \emph{paroxetine}) whose embedding vectors are more similar to the ``Mental Disorders" system vector than all others. \emph{Fluoxetine} is an anti-depressant medication solely used to treat ``Mental Disorders", so we would expect its vector to be more similar to this system vector than, say, the system vector representing ``Diseases of the Skin and Subcutaneous Tissue". 

Some drugs treat or prevent diseases in \emph{n} multiple medical field. For a field, such a drug is classified as being accurately predicted if the field's system vector is amongst the \emph{n} most similar system vectors. For instance, \emph{valproic acid} is an anti-convulsant used to treat both mental disorders and those of the nervous system. ``Mental Disorders'" \emph{System Vector Accuracy} would take into account whether its system vector was one of the \emph{n=2} most similar system vectors. For further illustration, Table ~\ref{tab:sysvectoy} shows an example SysVec calculation.



\subsection{Comparing Scores}

\paragraph{Comparing Sets of Embeddings}We calculated the mean scores for an embedding set, only including embeddings with corresponding ICD9 values and present in all of the compared sets. For the MCSM and MRM scores, we conducted two-way paired t-tests between the scores from each embedding set, adjusted with the Bonferroni correction. For the binary Bootstrap and SysVec scores, we judged statistical significance by calculating z-scores and their corresponding Bonferroni corrected p-values. 

A negative control set of embeddings was constructed by taking the embeddings from Beam et al \shortcite{beamClinicalConceptEmbeddings2018} and randomly arranging which clinical concepts an embedding corresponds to. 

\paragraph{Comparing Fields of Medicine} As the embeddings from Beam et al \shortcite{beamClinicalConceptEmbeddings2018} are most recent, trained on the most data, and have significantly higher scores than the other embeddings compared, we used these embeddings to compare scores from the different fields of medicine. This set also contained the most embeddings, allowing more embeddings from each field to be compared. 

We sought to determine whether a field of medicine's embeddings were significantly worse or better than the average. As such, for each field of medicine we calculated the mean score from each evaluation method. We then used statistical tests to compare a field's scores from a given evaluation method with the same scores from all other fields. For MCSM and MRM scores we used two-tailed t-tests, and for Bootstrap and Sysvec, z-scores, all corrected with the Bonferroni correction. 

To aggregate a medical field's results, we calculated a `net score' by taking how many of the four method's scores were significantly above the mean, minus how many were significantly below. We found this more interpretable than other methods such as aggregating normalized scores. 

%
%
%Our work seeks to determine if embeddings for one medical system are worse or better than others, or if those from a set of embeddings are worse or better. To do this, we must consider the scores generated using five different metrics, four different embeddings, across the seventeen medical systems. However, the scores from five metrics are not obviously convertible to a common score.  
%
%For now, we will assume our results are normally distributed; this may not be unreasonable as the processes underlying the quality of embeddings - the use of words representing clinical concepts in texts - stem from a natural process (human writing word choice).
%
%To compare the relative scores for each medical systems, for each evaluation method we calculate the mean score for the system's embeddings in each embedding set. We then conducted a paired two-tailed t-test for four pairs. Each pair contains a system's mean score vs the mean of all scores, for one embedding set. We then compare the relative difference between these means, express it as a percentage, and report whether this was significant at p \textless 0.05. 
%
%We repeat the same steps to compare the sets of embeddings. This time, we calculate the mean scores from each medical system for each evaluation method, and conduct the paired two-tailed t-tests on seventeen pairs for each medical system. The pairs are the mean score from all the embedding sets, against the score from the given embedding set. Again, we calculate the set's score relative to mean of all sets, and the significance as above. 
%
%After observing that the embeddings from Beam et al outperform the other embedding sets, we then use only these embeddings to evaluate the medical systems by the MCSM method. We chose this method  as it resulted in the highest number of significant differences, and is able to calculate scores for all systems. 
%
%Choosing one embedding set allows more embeddings to be compared, as previous scores were only calculated on embeddings shared by all embedding sets. As well, choosing one evaluation method  allows statistical analysis to be performed on the individual embedding scores instead of means. As such, for each medical system we then perform a one-sided two-tailed t-test between its embeddings and the scores from all relevant embeddings. We then repeated the same but only for the embeddings representing CUIs that were overlapping between the four embedding sets. This investigates the effect of restricting which CUIs are included.  

\section{Results}


\subsection{Differences Between Sets of Embeddings}
Comparing the sets of embeddings (Table ~\ref{tab:allembedresults}) shows consistent differences. BeamCui2Vec500's scores are the highest across all methods, and this difference is very significant, with p-value $\ll 10^-5$ after Bonferonni correction. The ChoiClaims300 embeddings seem next best, and the remaining sets still have much higher scores than those of the negative control. 


\begin{table*}[h]
	\begin{center}
		\begin{tabular}{lcccc}
			Embedding Set &MRM 	        &MCSM              &Bootsrap 	  &SysVec \\
			\hlineB{4}
			Negative Control& 0.02 & 1.24 & 0.05 & 0.35 \\
			\hline	            
			DeVine200       & 0.24 & 5.14 &	0.27 & 0.79 \\
			\hline
			ChoiClaims300   & 0.43 & 5.34 &	0.42 & 0.80 \\
			\hline 
			ChoiClinical300	& 0.33 & 4.49 &	0.42 & 0.74 \\
			\hline
			BeamCui2Vec500	& 0.52 & 6.39 & 0.67 & 0.90 \\
		\end{tabular}
		\caption{Mean scores for embedding sets for each evaluation method. See Methods section for abbreviations}
		\label{tab:allembedresults}
	\end{center}
	
\end{table*}


\subsection{Differences Between Medical Systems}

Differences are also observed between embeddings from the various fields of medicine as represented by the ICD-9 systems (Table ~\ref{tab:allsystemresults}). For instance, embeddings related to the medical field Infections and Parasitic Disease have scores significantly above the mean for all four evaluation methods, while those of the field Symptoms, Signs, and Ill-defined Conditions are significantly below for three. Due to a smaller number of documented drug-disease relationships across two medical fields, scores were not calculated with those methods using these relationships.  

\begin{table*}[!h]
	\begin{tabular}{lccccc}
		ICD-9 Systen                                                                                                          & MRM           & MCSM           & Bootstrap     & SysVec        & 	Net Score \\
		\hlineB{4}
		All Systems (Negative Control)                                                                                        & 0             & 1.08           & 0.04          & 0.25          & -                                \\
		\hline
		All Systems                                                                                                           & 0.55          & 8.07           & 0.89          & 0.63          & -                                \\
		\hline
		Infectious and Parasitic Diseases       & \textbf{0.45} & \textbf{7.72}  & \textbf{0.93} & \textbf{0.92} & +4                               \\
		\hline
		Neoplasms                                                                                                             & \textbf{0.62} & \textbf{9}     & 0.94          & 0.55          & +2                               \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Endocrine, Nutritional and Metabolic \\Diseases,  and Immunity Disorders\end{tabular} & \textbf{0.44} & \textbf{5.64}  & 0.89          & 0.53          & -2                               \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Diseases of the Blood \\ and Blood-forming Organs\end{tabular}                            & \textbf{0.31} & \textbf{4.36}  & 0.82          & 0.79          & -2                               \\
		\hline
		Mental Disorders                                                                                                      & 0.53          & \textbf{9.34}  & 0.96          & \textbf{0.83} & +2                               \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Diseases of the Nervous System \\ and Sense Organs\end{tabular}                           & \textbf{0.76} & \textbf{8.44}  & 0.87          & \textbf{0.33} & +1                               \\
		\hline
		Diseases of the Circulatory System                                                                                    & 0.59          & 8.12           & \textbf{0.96} & \textbf{0.72} & +2                               \\
		\hline
		Diseases of the Respiratory System                                                                                    & \textbf{0.36} & \textbf{5.85}  & 0.94          & \textbf{0.82} & +1                               \\
		\hline
		Diseases of the Digestive System                                                                                      & \textbf{0.61} & 7.93           & \textbf{0.77} & 0.62          & 0                                \\
		\hline
		Diseases of the Genitourinary System                                                                                  & \textbf{0.61} & \textbf{6.82}  & 0.86          & 0.58          & 0                                \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Complications of Pregnancy, \\ Childbirth, and the Puerperium\end{tabular}                & \textbf{0.51} & \textbf{10.27} & -             & -             & 0                                \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Diseases of the Skin \\ and Subcutanous Tissue\end{tabular}                               & \textbf{0.37} & \textbf{5.1}   & 0.81          & 0.58          & -2                               \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Diseases of the Musculoskeletal \\System and Connective Tissue\end{tabular}              & \textbf{0.47} & 8.22           & 0.88          & \textbf{0.29} & -2                               \\
		\hline
		Congenital Anomalies                                                                                                  & 0.5           & \textbf{6.24}  & 0.73          & 0.73          & -1                               \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Certain Conditions Originating\\ in the Perinatal Period\end{tabular}                    & \textbf{0.48} & \textbf{9.84}  & -             & -             & 0                                \\
		\hline
		\begin{tabular}[c]{@{}l@{}}Symptoms, Signs,  and \\ Ill-defined   Conditions\end{tabular}                           & \textbf{0.26} & \textbf{2.68}  & \textbf{0.77} & 0.56          & -3                               \\
		\hline
		Injury and Poisoning                                                                                                  & \textbf{0.59} & \textbf{9.09}  & \textbf{0.75} & \textbf{0}    & 0                               
	\end{tabular}
	\caption{Comparison of mean scores using different evaluation methods for the fields of medicine as represented by their ICD-9 system. Significant differences below or above the mean of all other systems are bold (p-value \textless 0.05 after Bonferroni correction). Net score is the number of these significant differences above that mean minus the number below. A system's  score is not calculated if there are fewer than ten examples for a method. See Methods section for evaluation method abbreviations}
	\label{tab:allsystemresults}
\end{table*}


%
%
%
%
%
%
%
%
%
%\begin{table*}[h]
%	\begin{center}
%	\label{tab:allsystemresults}
%	\begin{tabular}{l|c|c|c|c|c}
%		ICD9 Medical System& MRM (\%)                           & MCSM (\%)                         & SimCor (\%)                        & Bootstrap (\%)                    & SysVec (\%)                     \\
%		\hline
%		Infections           & -21                       & +41                         & \blu{-34} & \ora{+35} & +12                        \\
%		Cancers              & \ora{+19}  & +2                         & \ora{+51}  & +38                        & -4                        \\
%		Endocrine            & +3                         & \blu{-12} & \ora{+20}  & +23                        & -10                         \\
%		Blood diseases       & -7                        & \blu{-29} & -2                         & -6                       & +10                        \\
%		Mental disorders     & \ora{+23}   & \ora{+67}  & -45                        & +25                         & \ora{+33} \\
%		Nervous system       & \ora{+61}   & \ora{+64}  & -3                        & +17                        & \ora{+41} \\
%		Cardiovascular       & \ora{+12}  & \ora{+64}  & +19                         & +22                        & \ora{+38} \\
%		Respiratory          & -13                         & +7                         & \blu{-24} & \ora{+40} & -7                        \\
%		Digestive            & +24                         & -7                        & \blu{-42} & -7                       & -39                       \\
%		Genitourinary        & +29                         & +8                          & -15                        & +7                        & \ora{+18} \\
%		Pregnancy            & -22                        & \blu{-56}  &                               &                              &                              \\
%		Skin                 & \blu{-28} & -20                        & -31                        & +18                         & +8                        \\
%		Musculoskeletal        & +5                         & +13                          & \ora{+47}  & +6                        & \ora{+31} \\
%		Congenital           & -15                         & -13                        &                               & +11                        & \ora{+52} \\
%		Perinatal            & -38                        & \blu{-57} &                               &                              &                              \\
%		Ill-defined          & -18                        & -28                        & -24                        & -18                       & +5                        \\
%		Injury and poisoning & -14                        & \blu{-44} & \ora{+84}  & -12                       & +13                       
%	\end{tabular}
%	\caption{Percentage difference of a medical system's embeddings vs the mean score for all considered embeddings. Significant (paired t-test p \textless 0.05) scores are in orange (above mean) and blue (below). See Methods section for method abbreviations. Blank values represent no scores could be calculated for a system with that method.}
%\end{center}
%\end{table*}
%
%\subsection{Differences when Constraining Number of Embeddings}
%\red{Essentially take this out. Can be added later on or discussed in a paper. }
%In Table \ref{tab:onlybeamonlyMCSM} we focus on MCSM scores from the best performing set of embeddings, those from Beam et al. We observe results that are somewhat similar to the main  analysis. Embeddings related to mental disorders and the nervous system again perform well. Cardiovascular embeddings are better only when the more restrictive set of overlapping embeddings are considered. We note large differences for systems that had few overlapping embeddings but many only in the Beam et al embeddings, such as those related to pregnancy and injury and poisonings.  While these results descriptively seem to match for many systems, metrics indicate poor correlation. Assuming normality, Pearson's coefficient is only 0.16, while not assuming normality and using Spearman's we get only 0.01. 


%\begin{table*}[h]
%	\label{tab:onlybeamonlyMCSM}
%	\begin{tabular}{l|c|c|c|c|c|c|c}
%		& \multicolumn{3}{c|}{All Relevent Embeddings} & \multicolumn{3}{c|}{Overlapping Embeddings} &            \\
%		\hline
%		ICD9 System          & MCSM  & Examples & Difference (\%) & MCSM & Examples & Difference (\%) & Different? \\
%		\hline
%		All                  & 8.07  & 16351    & 0               & 5.60 & 2884     & 0               & Yes        \\
%		Infections           & 7.72  & 2261     & \blu{-4}       & 6.84 & 334      & \ora{+22}        & Yes        \\
%		Cancers              & 9.00  & 1194     & \ora{+12}        & 4.47 & 116      & \blu{-20}       & Yes        \\
%		Endocrine            & 5.64  & 545      & \blu{-30}       & 3.98 & 193      & \blu{-29}       & Yes        \\
%		Blood Diseases       & 4.36  & 199      & \blu{-46}       & 3.67 & 81       & \blu{-34}       & Yes        \\
%		Mental Disorders     & 9.34  & 662      & \ora{+16}        & 7.67 & 165      & \ora{+37}        & Yes        \\
%		Nervous              & 8.44  & 1787     & \ora{+5}         & 7.27 & 434      & \ora{+30}        & Yes        \\
%		Cardiovascular       & 8.12  & 869      & +1               & 7.74 & 307      & \ora{+38}        & No         \\
%		Respiratory          & 5.85  & 405      & \blu{-27}       & 4.64 & 132      & \blu{-17}       & Yes        \\
%		Digestive            & 7.93  & 852      & -2              & 4.62 & 210      & \blu{-18}       & Yes        \\
%		Genitourinary        & 6.82  & 606      & \blu{-15}       & 5.75 & 210      & +3               & Yes        \\
%		Pregnancy            & 10.27 & 1325     & \ora{+27}        & 2.47 & 10       & \blu{-56}       & Yes        \\
%		Skin                 & 5.10  & 305      & \blu{-37}       & 4.01 & 102      & \blu{-28}       & Yes        \\
%		Musculoskeletal        & 8.22  & 1041     & +2               & 5.24 & 168      & -6              & Yes        \\
%		Congenital Anomolies & 6.24  & 457      & \blu{-23}       & 5.05 & 101      & -10             & Yes        \\
%		Perinatal            & 9.84  & 310      & \ora{+22}        & 2.49 & 11       & \blu{-55}       & Yes        \\
%		Ill-defined          & 2.68  & 558      & \blu{-67}       & 2.81 & 224      & \blu{-50}       & No         \\
%		Injury and Poisoning & 9.09  & 2975     & \ora{+13}        & 2.42 & 86       & \blu{-57}       & Yes             
%	\end{tabular}
%\caption{Comparison of MCSM scores using the embeddings from Beam et al when considering all relevant embeddings, and only those that are overlapping with the other sets of embeddings. Includes the mean MCSM score for a system, number of examples per system, and the percentage difference vs mean. Significant differences are shown in orange/blue for above/below at p \textless 0.05. The final column is whether the scores of all embeddings vs overlapping embeddings are expected to be from a different population with p\textless 0.05.}
%\end{table*}

\section{Discussion and Future Direction}
To our knowledge, this is the first investigation into whether clinical concept embeddings from a given field of medicine perform relatively well or poorly compared to others. We conducted this investigation comparing available sets of such embeddings, using a variety of previously described intrinsic evaluation methods in addition to a new one. Given that one set of embeddings performed better than others, we used this set to compare the different fields of medicine, and found significant results between various fields. 

The superior performance of one set of embeddings - those from Beam et al \shortcite{beamClinicalConceptEmbeddings2018} - are consistent with the depth and breadth of data used to train these embeddings. Training used three different types of data, including that from health insurance claims, clinical narratives, and full texts from medical journals. The size of the dataset was also much larger than that of the others . Our work validates their findings that their embeddings offer the best performance. However, it would be interesting to also consider the recent clinical concept embeddings developed by \cite{xiangTimesensitiveClinicalConcept2019}. They use a similar amount of data (50 million) as Beam et al, using a large dataset from electronic health records, and apply a novel method to incorporate time-sensitive information. At the time of submission, we were unable to obtain their embeddings, and so leave this comparison to future work.

Examining the differences between fields of medicine, we note that the poor performance of embeddings from the system ``Symptoms, Signs, and Ill-defined Conditions" may support validity of the results. This collection of miscellaneous medical conditions would not be expected to have the intrinsic vector similarity and cohesion evaluated by our evaluation methods.   

Further work may explore why the other systems have varied performance. We wonder if the observed results correlate with possible distinctiveness of the various medical fields. For example, the best performing system was ``Infectious and Parasitic Diseases". The conditions in this field are often unambiguous - a pathogen like \emph{influenza virus} has little other meaning - and the drugs used for these diseases tend to be similarly specific. On the other hand,  poorly performing systems such as ``Diseases of the Skin and Subcutaneous Tissue" and "Diseases of the Musculoskeletal Systems and Connective Tissue" often utilize immunosuppressant medications that are used across many fields of medicine. Future work could investigate this conjecture by comparing scores when restricting what clinical concepts are compared, such as only common or distinct medications. 

This work evaluated embeddings using intrinsic measures of embedding quality. This presents some advantages, but also the most obvious limitation and direction for future work. These intrinsic methods allowed a consistent evaluation to be carried out between medical fields, and allowed a wide variety of embedding sets to be compared. The methods all evaluate qualities that well-trained embeddings should have, though still represent artificial use-cases. Evaluating these embeddings on extrinsic, down-stream tasks may provide more practically relevant comparisons. However, these tasks will need to be comparable and available for multiple medical fields. For instance, the recent work by Xiang et al \shortcite{xiangTimesensitiveClinicalConcept2019} compared embeddings trained by different methodologies on a task predicting the onset of heart failure \cite{rasmyStudyGeneralizabilityRecurrent2018}. This would be an appropriate task to judge embeddings from ``Diseases of the Circulatory System";  others would be needed for other systems. We also plan to investigate the validity of these intrinsic evaluation methods by comparing them to extrinsic results. 

Another future direction could be to investigate what could be done to improve performance in the fields with lower scores. For instance, Zhang et al \shortcite{zhangAdaptingWordEmbeddings2018} used domain adaptation techniques for psychiatric embeddings, and this could also be carried out for those systems we identified as doing poorly. Alternatively, one could train embeddings solely on data from one field of medicine and investigate how this affects performance. 

%
%domain adaptation-trained embeddings in this project would help quantify this technique's potential benefit, but we were unable to obtain their embeddings. It would be interesting to carry DA out on embeddings from a poorly performing medical system. Or, for a larger project, medical-field-specific embeddings could be trained, and compared to the ones used in this project, in order to determine the scale of possible improvement. 


%
%
%\subsection{Differences Between Medical Systems}
%
%
%Why some of these systems seem to perform better is unclear. Both mental and neurological disorders relate to the brain, but their respective fields of medicine are otherwise dissimilar across frequency, language, and specificity of diseases.  If we take the numbers of compared embeddings as a proxy for how common these medical system's conditions are, the well performing systems are neither particularly common or uncommon. As the frequency a concept occurs in training data may impact the quality of its embedding, this may be something to examine going forwards. 
%
%Strengths of the project include an objective way of relating a clinical concept to a field of medicine. As well, the combination of multiple evaluation metrics and sets of embeddings encompasses most of the prior work in this topic.  However, these complexities also lead to weaknesses. First and foremost, the combination of different metrics, embedding sets, and medical systems creates a daunting task for statistical analysis, and this could likely be improved by considering advanced techniques such as non-parametric equivalents to ANOVA and ensuing post-hoc analysis. 
%
%Additionally, we see that there is a difference in the results when fewer or greater numbers of embeddings are considered for a given system. The initial analysis only considered embeddings that were common to all embedding sets, significantly restricting the numbers of embeddings considered compared to the relevant concepts within the most extensive \emph{cui2vec} embeddings from Beam et al. One possible cause of the difference may be simply sampling too few embeddings; for some systems, only a few dozen examples had embeddings in all four embedding sets. However, the score difference may also be the result of selecting less common medical terms; the set of overlapping embeddings likely represent more common clinical concepts if all four training methods include them. 
%
%This last point is important to consider when assessing the project's results. The differences we find between medical systems could be due to how many rare clinical concepts a system has; these rare terms may have had fewer chances to be trained well due to seldomly occurring in the training data. This may or may not be a confounder. A future researcher using the embeddings in a particular field of medicine may be using embeddings from a variety of concepts in her field, including rare ones, in which case worse performance due to a field containing rare terms would be desired. Conversely, applications may only consider common terms in a field, in which case these rare terms would indeed be confounding. This limitation can be addressed, as will be discussed below. However, our main analysis does only consider CUIs in all four embedding sets, which may itself limit the rarity of which embeddings are compared. 
%
%\subsection{Differences Between Sets of Embeddings}
%
%Another goal of the project was to assess the quality of each set of embeddings. We see that Beam et al's \emph{cui2vec} embeddings clearly come out ahead. This is expected, as these embeddings are the most recent, are trained on the largest amount of data, and are trained on three different types of data. They also contain the largest number of embeddings. While we are only comparing four sets, our results may suggest that health insurance claim data is particularly useful to train embeddings, given that the two sets of embeddings containing this data performed better. 
%
%These results also suggest that our use of ICD9 systems as a relationship to judge embedding vectors - such as whether embeddings from a given system are near each other - are a contribution to the field. 

%
%\subsection{Comparing Evaluation Methods}
%Our work also allows us to compare embedding evaluation methods. It is difficult to judge them concretely, as we do not have a gold standard to tell us what results a method should produce.
%
%Yu et al's method of comparing correlation with similarity as judged by resident physicians was hampered by having a small number of judged similarities (around 500) which led to few comparisons per system in our deployment. Our new  method utilizing `system vectors' observed similar differences between medical systems as found by other methods. However, it found very small differences between sets of embeddings. Neither of these two methods found the \emph{cui2vec} embeddings to be significantly better than the others. If we assume this difference should be found, this may suggest these two methods are inferior. Our implementation of Yu et al's method could likely be slightly improved; some of the human similarity comparisons are not used as the CUIs chosen are not found within our embeddings, so could be changed to very related ones that are, eg Diabetes to Diabetes Mellitus.  
%
%The two methods from Choi et al produced similar results. This could likely be expected, as they are similar, both employing discounted cumulative gain to score whether neighbours are related according to differing known relations. Their results are broadly in line with Beam's methodology, which uses similar known relations but instead judges them against a bootstrap of random relations. 
%
%Of note, besides Yu's method, all other methods can vary depending on what known relation is used. For instance, Beam et al's method as implemented in our project evaluates known relations between drugs and the diseases they treat or prevent, but it could instead be changed to evaluate whether two concepts are in the same medical system. This is another variable affecting our results. One can understand our results as testing just one configuration of a method, as opposed to evaluating them more broadly.

%\section{Future Work}
%\paragraph{Improving the current project}: There are multiple ways to improve the current project. Different statistical analyses, including non-parametric methods, could be used to consider all raw scores. This would replace the current analysis which often calculates mean scores, losing information such as examples per system. These different statistical methods may allow aggregation of evaluation model results, allowing a more definitive answer as to whether embedding sets or medical systems lead to significant differences.  
%
%It was difficult to compare evaluation methods due to not knowing what results to expect. A possible workaround is to construct negative controls, for both systems and embeddings. For instance, random ranges of ICD9 codes could be used to construct such systems, and randomized CUIs used for such embeddings. Indeed, including the results of these controls could make other results more interpretable. Positive controls would likely take more work to construct, but could also be possible. 
%
%Currently, the dictionary between UMLS CUIs and ICD9CM codes contains around 40,000 entries. As such, many CUIs could not be used, despite representing concepts very related to those in the dictionary. Generating a larger dictionary automatically would be useful, and could be feasible given the requirement that they only be labelled into medical systems and their wide range of ICD9 codes. ICD9 is itself an old system, and the newer ICD10 systems could instead be used, or the ICD11 system about to be released this year.  
%
%As discussed previously, a possible confounder is whether embeddings from a given medical system perform worse due to containing rare conditions. We could investigate this only comparing embeddings that represent common conditions. We intended this to be part of the current project, but deferred it. Frequencies of ICD9 codes are not readily available except within public health databases, which are considered sensitive and require formal applications to access. This likely could not be acquired in the time-frame of this project, though they represent an interesting avenue for future work. 
 
%\paragraph{Possible extensions:}


%Finally, in this project we quantify embedding quality by their geometric qualities against known relations. A more `real-world' evaluation could be attempted carrying out actual NLP tasks on documents from different medical systems to understand their relative performance from a more applied perspective. For instance, we could use perform NLP tasks using the embeddings on articles about conditions from the various medical systems, using Wikipedia or a medical encyclopaedia like UpToDate. Or, to be even more applied, NLP tasks could be evaluated on medical documents produced by physicians of different specialities, an opportunity we may soon have access to from an ongoing project. 
%



%If the proposed methodology is implemented easily and quickly, a possible extension will be determine the feasibility of training new embeddings based only on psychiatric data, such as using a subset of the matrix used by Choi et al's \shortcite{choiLearningLowDimensionalRepresentations2016}; we could try only using the portion of the matrix with terms related to psychiatry. 
%
%Alternatively, it may be interesting to use the embeddings from prior work to carry out various document-level summarization techniques, and compare doing so for psychiatric vs non-psychiatric documents. For instance, this could be done on articles from Wikipedia describing popular illnesses in and outside of psychiatry, or a similar set of articles from the medical practice manual and learning resource UpToDate.
%
%In the longer term, this project may be applicable to a separate project applying NLP and ML techniques to a large BC Cancer clinical dataset consistency of the medical records of around 50,000 patients and their free text medical documents, numbering in the 100,000's. This dataset may allow both evaluation or training when available in the future. 
\newpage
\bibliographystyle{acl_natbib}
\bibliography{my_library}



%\newpage
%\onecolumn
%\section{Appendix}
%This project relies on around 1000 lines of code, which seems infeasible to print out. 
%
%As such, over the following pages, I will include code from the three analyses I wrote from scratch. However, this is by no means the extent of the new programming required for the project. 
%
%Full code of the project is available on my \href{https://github.com/jjnunez11/MedicalEmbeddingsByField}{Github}. However, I would rather not make it public. I'm happy to add Giuseppe or Linzi as collaborates so they can view it. In my email of the report to Giuseppe, I'll also attach a zip of the repo with the code. 
%
%\lstset{language=Python}
%\lstset{frame=lines}
%\lstset{caption={Code implementing Yu et al's Analysis}}
%\lstset{label={lst:code_direct}}
%\lstset{basicstyle=\small}
%
%\lstinputlisting[language=Python]{code_examples.py}

\end{document}
