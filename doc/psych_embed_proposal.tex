%
% Based on File hlt-naacl2004.tex
%


\documentclass[10pt]{article}
\usepackage{hltnaacl04}
\usepackage{times}
\usepackage{latexsym}


\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Project Proposal: Do Word Embeddings Trained on General Medical Data Work for Psychiatric Tasks?}

\author{John-Jose Nunez\\
  Depts. of Psychiatry and Computer Science, UBC\\
  {\tt jjnunez11@gmail.com} 
}
\date{}

\begin{document}
\maketitle
%\begin{abstract}
%  This document contains the instructions for preparing a camera-ready
%  manuscript for the proceedings of HLT-NAACL-2004. The document itself conforms
%  to its own specifications, and is therefore an example of what
%  your manuscript should look like.  Authors are asked to conform to
%  all the directions reported in this document.
%\end{abstract}

%\section{Credits}
%
%This document has been adapted from the instructions for the
%ACL-2002 proceedings by Dekang Lin and Eugene Charniak.  It was
%adapted in turn from the ACL-02 proceedings by Norbert Reithinger,
%Giorgio Satta, and Roberto Zamparelli. The ACL-01 instructions was
%elaborated from similar documents used for previous editions of the
%ACL and EACL annual meetings. Those versions were written by several
%people, including John Chen, Henry S. Thompson and Donald
%Walker. Additional elements were taken from the formatting
%instructions of the Xth {\em International Joint Conference on
%Artificial Intelligence}.

\section{Introduction and Proposed Contribution}

\subsection{Background}

Recent work has applied natural language processing and machine learning to various predictive and classification tasks in medicine, such as EXAMPLE and EXAMPLE. Various methods are used to convert words to low-dimension vector representations which can then be used in these applications. These vectorizations are produced by training models on large corpora. For example, the popular \emph{word2vec} system \cite{mikolovEfficientEstimationWord2013} initially trained embeddings using a skip-gram model trained on a Google News corpus containing around six billion tokens. Due to considerable differences between the language of medical text and general English writing, prior work has trained  medical embeddings using medical sources. 

Recent approaches in this vein include De Vine et al \shortcite{devineMedicalSemanticSimilarity2014} who trained embeddings for medical concepts in the Unified Library Management System (ULMS) using journal abstracts from MEDLINE and clinical patient records. They then used these embeddings to compare predicted word similarity against human-judgements. Minarro-Gimenez et al \shortcite{minarro-gimenezExploringApplicationDeep2014} trained using medical manuals, articles, and Wikipedia articles, comparing predicted word similarity between medications against the National Drug File - Reference Terminology (NDF-RT) ontology. Choi et al \cite{choiLearningLowDimensionalRepresentations2016} improve on this work by learning on health record data consisting of raw text from clinical notes mapped to concepts from UMLS. In their yet unpublished work, Beam et al \cite{beamClinicalConceptEmbeddings2018} use an ``extremely large'' database of clinical notes, insurance claims, and full journal texts, and develop a new system termed ``cui2vec'', mapping concepts into a set of identifiers based on UCMLS, and then training on occurrences on these concepts with each other in a certain window length. 

All of the above examples were both trained and tested on general medical data, from all fields of medicine. It is unclear whether these models will perform as well when applied to a specific field of medicine. This may be especially true in the medical speciality of psychiatry, the field of medicine concerned with mental illness such as depression or schizophrenia. Prior work has shown that psychiatric symptoms are often described in a long, varied, and subjective manner \cite{forbushSittingPinsNeedles2013} which may present a particular challenge for NLP tasks. 

Prior work has explored whether domain adaptation (DA), techniques to adapt data from other domains to work on a target, can improve performance when applied to this sub-domain of psychiatry. Lee et al \cite{leeLeveragingExistingCorpora2018} used these techniques to improve the task of de-identifying psychitric notes. Zhang et al \cite{zhangAdaptingWordEmbeddings2018} then applied DA to word embeddings trained from various sources: general language (Wikipedia), medical articles (MEDLINE), non-psychiatric medical notes, and data from an online forum used by mental health patients, testing on a dataset of psychiatric notes, showing some improvement by using DA. 

\subsection{Contribution}

This project seeks to build on these prior works advancing the application of word embedding techniques in psychiatry. Specially, we will seek to determine whether embeddings trained on general medical data perform as well on psychiatric content as they do on other domains within medicine. From our knowledge, this has not been examined previously. We will compare various prior techniques and embeddings, to determine if techniques such as DA, or training on larger data-sets, improve applicability to psychiatry. This will contribute to future applications of NLP to psychiatry by investigating whether or not embeddings trained on domain-specific data may be required. This will either give impetus to the collection of a large psychiatric dataset and the training of our own embeddings, or will reassure future work that general medical embeddings are sufficient. 


\section{Proposed Methodology}

Generally, the project will to deploy the embeddings of prior projects, reusing their evaluation methods to compare these metrics when applied to psychiatric content compared to other fields of medicine. Psychiatric content (eg drugs, diagnoses) will be compared to those of general medical applicability, and other fields of medicine such as general practice, a general speciality like internal medicine, and another speciality that may have very specific terminology, ophthalmology. Performance metrics that can be compared against all embeddings and techniques will be used to determine if any improve domain applicability. 

The embeddings/techniques to be replicated include:
\begin{itemize}
	\item De Vine et al's \shortcite{devineMedicalSemanticSimilarity2014} embeddings trained on medical records and abstracts.  
	\item Minarro-Gimenez et al's \shortcite{minarro-gimenezExploringApplicationDeep2014} embeddings trained on medical manuals and articles, Wikipedia.
	\item Choi et al's \shortcite{choiLearningLowDimensionalRepresentations2016}'s two sets of embeddings trained differently using raw data mapped to a matrix based on UMLS techniques.  
	\item Zhang et al's \shortcite{zhangAdaptingWordEmbeddings2018} best performing embeddings using domain-adaptation techniques. 
	\item Beam et al's cui2vec embeddings trained on health insurance claims and full journal texts. 
\end{itemize}

The evaluation techniques to be replicated used to compare psychiatric outcomes vs others:
\begin{itemize}
	\item De Vine et al's \shortcite{devineMedicalSemanticSimilarity2014}'s evaluation framework, comparing predicted vector similarity against human judgements, comparing psychatric concepts vs others judged.  
	\item Minarro-Gimenez et al's \shortcite{minarro-gimenezExploringApplicationDeep2014}'s metric of predicting relationships between drugs based on the NDF-RT. This easily allows comparison between psychiatric drugs against those of other fields.
	\item Choi et al's \shortcite{choiLearningLowDimensionalRepresentations2016} Conceptual Similarity Property, comparing predicted vector similarity with whether concepts are neighbouring in UMLS. We will extended the concepts tested to include psychiatric and non-psychiatric ones
	\item Choi et al's \shortcite{choiLearningLowDimensionalRepresentations2016} Medical Relatedness Property, comparing predicted vector similarity with relatedness according to NDF-RT and the ICD9 groupings, based on these database's item relations such as ``may-treat'' and ``may-prevent''. Subsets can again be specified.
	\item Beam et al's \shortcite{beamClinicalConceptEmbeddings2018} statistical score based on whether known similarities in UMLS, NDF-RT and other work are predicted correctly in at least 95\% of bootstrapped samples of pairs of concepts. 

In order to determine which psychiatric and non-psychiatric terms should be compared, the most common concepts shall be used. For instance, we will compare the most commonly prescribed psychatric and non-psychatric drugs, or the most common diagnoses, based on prior epidemiology, in order to compare common, well described concepts.

\subsection{Current Data Availability}

Of the five works mentioned above, two have their data publicly available for download, one does not but has previously shared data with other authors, one is fully published so will likely share, and one is planning to share, but only when they are published. Relevant authors have been or will be contacted. 

\subsection{Project Flexibility and Extensibility}

At a minimum, this project will use the available embeddings, and implement the metrics whose code is available, or whose description is sufficient to allow replication. An extensible system will be used such that future embeddings, when available, can be easily incorporated. It is expected that, even if not all embeddings are available by the project due data, the implementation of the embeddings and evaluation metrics available will be the majority of the work for the total project, and will yield a sizeable contribution. 

If the proposed methodology is implemented easily and quickly, a possible extension will be determine the feasibility of training new embeddings based only on psychiatric data based on prior work, such as using the matrix from Choi et al's work \shortcite{choiLearningLowDimensionalRepresentations2016} but only the portion of the matrix with terms related to psychiatry. 

Alternatively, it may be interesting to use the embeddings from prior work to carry out various document-level summarization techniques, and compare doing so psychiatric vs non-psychiatric documents. For instance, this could be done on articles from Wikipedia describing popular illnesses in psychiatry and non-psychiatry, or articles from the medical practice manual UpToDate.

%In the longer term, this project may be applicable for a separate project applying NLP and ML techniques to large BC Cancer clinical dataset consistency of medical records of around 50,000 patients and their free text medical documents, numbering in the 100,000's. This dataset may allow both evaluation or training when available in the future. 
\end{itemize}

\section{Expected Results}

Due to the uniqueness of psychiatry, we expect the various embeddings will generally perform worse when used for psychiatric concepts than those not in this speciality. We expected that the performance of the various embeddings/techniques will follow a similar pattern with their general performance. However, it would not be overly surprising if the embeddings trained on larger dataset may perform worse for psychiatric terms, as the psychiatric-specific meaning of a word may get ``drowned-out'' more in larger datasets. 
%
%
%\section{Format of Electronic Manuscript}
%\label{sect:pdf}
%
%For the production of the electronic manuscript you must use Adobe's
%Portable Document Format (PDF). This format can be generated from
%postscript files: on Unix systems, you can use {\tt ps2pdf} for this
%purpose; under Microsoft Windows, Adobe's Distiller can be used.  Note
%that some word processing programs generate PDF which may not include
%all the necessary fonts (esp. tree diagrams, symbols). When you print
%or create the PDF file, there is usually an option in your printer
%setup to include none, all or just non-standard fonts.  Please make
%sure that you select of the option of including ALL the fonts.  {\em
%  Before sending it, test your {\/\em PDF} by printing it from a
%  computer different from the one where it was created}. Moreover,
%some word processor may generate very large postscript/PDF files,
%where each page is rendered as an image. Such images may reproduce
%poorly.  In this case, try alternative ways to obtain the postscript
%and/or PDF.  One way on some systems is to install a driver for a
%postscript printer, send your document to the printer specifying
%``Output to a file'', then convert the file to PDF.
%
%For reasons of uniformity, Adobe's {\bf Times Roman} font should be
%used. In \LaTeX2e{} this is accomplished by putting
%
%\begin{quote}
%\begin{verbatim}
%\usepackage{times}
%\usepackage{latexsym}
%\end{verbatim}
%\end{quote}
%in the preamble.
%
%Additionally, it is of utmost importance to specify the US {\bf
%  US-Letter format} (8.5in $\times$ 11in) when formatting the paper.
%When working with {\tt dvips}, for instance, one should specify {\tt
%  -t letter}.
%
%
%\subsection{Layout}
%\label{ssec:layout}
%
%Manuscripts should have two columns to a page, in the manner these
%instructions are printed. The exact dimensions for a page on US-letter
%paper are:
%
%\begin{itemize}
%\item Left and right margins: 1in
%\item Top margin:1in
%\item Bottom margin: 1in
%\item Columns width: 3.15in
%\item Column height: 9in
%\item Gap between columns: 0.2in
%\end{itemize}
%
%
%\subsection{The First Page}
%\label{ssec:first}
%
%Center the title, (s) and affiliation(s) across both
%columns. Do not use footnotes for affiliations.  Do not include the
%paper ID number that was assigned during the submission process. 
%Use the two-column format only when you begin the abstract.
%
%{\bf Title}: Place the title centered at the top of the first page, in
%a 15-point bold font. Long title should be typed on two lines without
%a blank line intervening. Approximately, put the title at 1in from the
%top of the page, followed by a blank line, then the author's names(s),
%and the affiliation on the following line.  Do not use only initials
%for given names (middle initials are allowed). The affiliation should
%contain the author's complete address, and if possible an electronic
%mail address. Leave about 0.75in between the affiliation and the body
%of the first page.
%
%{\bf Abstract}: Type the abstract at the beginning of the first
%column.  The width of the abstract text should be smaller than the
%width of the columns for the text in the body of the paper by about
%0.25in on each side.  Center the word {\bf Abstract} in a 12 point
%bold font above the body of the abstract. The abstract should be a
%concise summary of the general thesis and conclusions of the paper.
%It should be no longer than 200 words.
%
%{\bf Text}: Begin typing the main body of the text immediately after
%the abstract, observing the two-column format as shown in 
%the present document. Type single spaced. 
%{\bf Indent} when starting a new paragraph. 
%
%{\bf Font:} 
%For reasons of uniformity,
%use Adobe's {\bf Times Roman} fonts, with 10 points for text and 
%subsection headings, 12 points for section headings and 15 points for
%the title. If Times Roman is unavailable, use {\bf Computer Modern
%  Roman} (\LaTeX2e{}'s default; see section \ref{sect:pdf} above).
%Note that the latter is about 10\% less dense than Adobe's Times Roman
%font.  
%
%\subsection{Sections}
%
%{\bf Headings}: Type and label section and subsection headings in the
%style shown on the present document.  Use numbered sections (Arabic
%numerals) in order to facilitate cross references. Number subsections
%with the section number and the subsection number separated by a dot,
%in Arabic numerals. Do not number subsubsections.
%
%{\bf Citations}: Follow the ``Guidelines for Formatting Submissions''
%to {\em Computational Linguistics\/} that appears in the first issue of
%each volume, if possible.  That is, citations within the text appear
%in parentheses as~\cite{Gusfield:97} or, if the author's name appears in
%the text itself, as Gusfield~\shortcite{Gusfield:97}. 
%Append lowercase letters to the year in cases of ambiguities.  
%Treat double authors as in~\cite{Aho:72}, but write as 
%in~\cite{Chandra:81} when more than two authors are involved. 
%Collapse multiple citations as in~\cite{Gusfield:97,Aho:72}.
%
%\textbf{References}: Gather the full set of references together under
%the heading {\bf References}; place the section before any Appendices,
%unless they contain references. Arrange the references alphabetically
%by first author, rather than by order of occurrence in the text.
%Provide as complete a citation as possible, using a consistent format,
%such as the one for {\em Computational Linguistics\/} or the one in the 
%{\em Publication Manual of the American 
%Psychological Association\/}~\cite{APA:83}.  Use of full names for
%authors rather than initials is preferred.  A list of abbreviations
%for common computer science journals can be found in the ACM 
%{\em Computing Reviews\/}~\cite{ACM:83}.
%
%The provided \LaTeX{} and Bib\TeX{} style files roughly fit the
%American Psychological Association format, allowing regular citations, 
%short citations and multiple citations as described above.
%
%{\bf Appendices}: Appendices, if any, directly follow the text and the
%references (but see above).  Letter them in sequence and provide an
%informative title: {\bf Appendix A. Title of Appendix}.
%
%\textbf{Acknowledgements} sections should go as a last section immediately
%before the references.  Do not number the acknowledgement section.
%
%\subsection{Footnotes}
%
%{\bf Footnotes}: Put footnotes at the bottom of the page. They may
%be numbered or referred to by asterisks or other
%symbols.\footnote{This is how a footnote should appear.} Footnotes
%should be separated from the text by a line.\footnote{Note the
%line separating the footnotes from the text.}
%
%\subsection{Graphics}
%
%{\bf Illustrations}: Place figures, tables, and photographs in the
%paper near where they are first discussed, rather than at the end, if
%possible.  Wide illustrations may run across both columns. Do not use
%color illustrations as they may reproduce poorly.
%
%{\bf Captions}: Provide a caption for every illustration; number each one
%sequentially in the form:  ``Figure 1. Caption of the Figure.'' ``Table 1.
%Caption of the Table.''  Type the captions of the figures and 
%tables below the body, using 10 point text.  


\bibliographystyle{acl}
\bibliography{my_library}{}

%
%\begin{thebibliography}{}
%	
%
%	
%
%\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
%Alfred~V. Aho and Jeffrey~D. Ullman.
%\newblock 1972.
%\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
%\newblock Prentice-{Hall}, Englewood Cliffs, NJ.
%
%\bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
%{American Psychological Association}.
%\newblock 1983.
%\newblock {\em Publications Manual}.
%\newblock American Psychological Association, Washington, DC.
%
%\bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
%{Association for Computing Machinery}.
%\newblock 1983.
%\newblock {\em Computing Reviews}, 24(11):503--512.
%
%\bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
%Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
%\newblock 1981.
%\newblock Alternation.
%\newblock {\em Journal of the Association for Computing Machinery},
%  28(1):114--133.
%
%\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
%Dan Gusfield.
%\newblock 1997.
%\newblock {\em Algorithms on Strings, Trees and Sequences}.
%\newblock Cambridge University Press, Cambridge, UK.
%
%\end{thebibliography}

\end{document}
